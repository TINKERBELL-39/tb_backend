"""
í†µí•© ê³ ê° ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ë§¤ë‹ˆì € - ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ (ê¸°ì¡´ êµ¬ì¡° ê¸°ë°˜ ìˆ˜ì •)
ë§ˆì¼€íŒ… ì—ì´ì „íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ ë¦¬íŒ©í† ë§
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from enum import Enum
import json
from datetime import datetime

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from shared_modules import (
    get_config,
    get_llm_manager,
    get_vector_manager,
    get_or_create_conversation_session,
    create_message,
    get_recent_messages,
    insert_message_raw,
    get_session_context,
    create_success_response,
    create_error_response,
    get_current_timestamp,
    format_conversation_history,
    load_prompt_from_file,
    PromptTemplate,
    get_templates_by_type
)

from customer_service_agent.config.persona_config import PERSONA_CONFIG, get_persona_by_topic
from customer_service_agent.config.prompts_config import PROMPT_META
from langchain.schema import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from shared_modules.queries import get_user_persona_info

logger = logging.getLogger(__name__)

class ConversationStage(Enum):
    """ëŒ€í™” ë‹¨ê³„ ì •ì˜"""
    INITIAL = "initial"                    # ì´ˆê¸° ì ‘ì´‰
    PROBLEM_IDENTIFICATION = "problem_identification"  # ë¬¸ì œ íŒŒì•…
    INFORMATION_GATHERING = "info_gathering"  # ì •ë³´ ìˆ˜ì§‘
    ANALYSIS = "analysis"                  # ë¶„ì„
    SOLUTION_PROPOSAL = "solution_proposal"  # í•´ê²°ì±… ì œì•ˆ
    FEEDBACK = "feedback"                  # í”¼ë“œë°± ìˆ˜ì§‘
    REFINEMENT = "refinement"              # ìˆ˜ì •
    FINAL_RESULT = "final_result"          # ìµœì¢… ê²°ê³¼
    COMPLETED = "completed"                # ì™„ë£Œ

class ConversationState:
    """ëŒ€í™” ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, conversation_id: int, user_id: int):
        self.conversation_id = conversation_id
        self.user_id = user_id
        self.stage = ConversationStage.INITIAL
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        
        # ğŸ”¥ ìˆ˜ì •: ê°„ì†Œí™”ëœ ì •ë³´ ìˆ˜ì§‘
        self.collected_info = {
            "business_type": None,           # ì‚¬ì—… ìœ í˜•
            "customer_issue": None,          # ê³ ê° ë¬¸ì œ/ë¶ˆë§Œ
            "customer_segment": None,        # ê³ ê° ì„¸ê·¸ë¨¼íŠ¸
            "current_situation": None,       # í˜„ì¬ ìƒí™©
            "desired_outcome": None,         # ì›í•˜ëŠ” ê²°ê³¼
            "urgency_level": None,           # ê¸´ê¸‰ë„
            "available_resources": None,     # ê°€ìš© ìì›
            "previous_attempts": None,       # ì´ì „ ì‹œë„
            "customer_data": None,          # ê³ ê° ë°ì´í„°
            "communication_channel": None,   # ì†Œí†µ ì±„ë„
            "timeline": None,               # í•´ê²° ê¸°í•œ
            "budget": None,                 # ì˜ˆì‚°
            "additional_context": {}
        }
        
        # ë¶„ì„ ê²°ê³¼
        self.analysis_results = {
            "primary_topics": [],
            "intent_analysis": {},
            "customer_sentiment": "neutral",
            "problem_category": None,
            "recommendations": []
        }
        
        # í•´ê²°ì±… ë° ì œì•ˆ
        self.solutions = []
        self.feedback_history = []
        self.refinements = []
        
        # ìµœì¢… ê²°ê³¼
        self.final_solution = None
        self.action_plan = None
        
        # ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ê¸°ë¡
        self.stage_prompts = {}
        
    def update_stage(self, new_stage: ConversationStage):
        """ë‹¨ê³„ ì—…ë°ì´íŠ¸"""
        self.stage = new_stage
        self.updated_at = datetime.now()
        
    def add_collected_info(self, key: str, value: Any):
        """ìˆ˜ì§‘ëœ ì •ë³´ ì¶”ê°€"""
        self.collected_info[key] = value
        self.updated_at = datetime.now()
        
    def add_feedback(self, feedback: Dict[str, Any]):
        """í”¼ë“œë°± ì¶”ê°€"""
        feedback["timestamp"] = datetime.now()
        self.feedback_history.append(feedback)
        self.updated_at = datetime.now()
        
    def is_information_complete(self) -> bool:
        """ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸ (ì™„í™”ëœ ì¡°ê±´)"""
        # ğŸ”¥ ìˆ˜ì •: í•„ìˆ˜ í•„ë“œ 2ê°œ + ì´ 3ê°œ ì´ìƒ
        essential_fields = ["business_type", "desired_outcome"]
        has_essentials = all(self.collected_info.get(field) for field in essential_fields)
        filled_count = len([v for v in self.collected_info.values() if v])
        return has_essentials and filled_count >= 3
        
    def get_completion_rate(self) -> float:
        """ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œìœ¨"""
        total_fields = len(self.collected_info)
        completed_fields = len([v for v in self.collected_info.values() if v])
        return completed_fields / total_fields if total_fields > 0 else 0.0

class CustomerServiceAgentManager:
    """í†µí•© ê³ ê° ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ê´€ë¦¬ì - ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ê³ ê° ì„œë¹„ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.config = get_config()
        self.llm_manager = get_llm_manager()
        self.vector_manager = get_vector_manager()
        
        # í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.prompts_dir = Path(__file__).parent.parent / 'prompts'
        
        # ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        self.knowledge_collection = 'customer-service-knowledge'
        
        # ê³ ê° ì„œë¹„ìŠ¤ í† í”½ ì •ì˜
        self.customer_topics = {
            "customer_service": "ê³ ê° ì‘ëŒ€ ë° í´ë ˆì„ ì²˜ë¦¬",
            "customer_retention": "ì¬ë°©ë¬¸ ìœ ë„ ë° ê³ ê° ìœ ì§€",
            "customer_satisfaction": "ê³ ê° ë§Œì¡±ë„ ê°œì„ ",
            "customer_feedback": "ê³ ê° í”¼ë“œë°± ë¶„ì„",
            "customer_segmentation": "ê³ ê° íƒ€ê²ŸíŒ… ë° ì„¸ë¶„í™”",
            "community_building": "ì»¤ë®¤ë‹ˆí‹° êµ¬ì¶•",
            "customer_data": "ê³ ê° ë°ì´í„° í™œìš©",
            "privacy_compliance": "ê°œì¸ì •ë³´ ë³´í˜¸",
            "customer_message": "ê³ ê° ë©”ì‹œì§€ í…œí”Œë¦¿",
            "customer_etc": "ê¸°íƒ€ ê³ ê° ê´€ë¦¬"
        }
        
        # ëŒ€í™” ìƒíƒœ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.conversation_states: Dict[int, ConversationState] = {}
        
        # ğŸ”¥ ìˆ˜ì •: ê°„ì†Œí™”ëœ ì§ˆë¬¸ í…œí”Œë¦¿ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.info_gathering_questions = {
            "business_type": "ì–´ë–¤ ì—…ì¢…/ì‚¬ì—…ì„ ìš´ì˜í•˜ê³  ê³„ì‹ ê°€ìš”?",
            "desired_outcome": "ì–´ë–¤ ê²°ê³¼ë¥¼ ì›í•˜ì‹œë‚˜ìš”?",
            "customer_issue": "í˜„ì¬ ì–´ë–¤ ê³ ê° ê´€ë ¨ ë¬¸ì œë‚˜ ì´ìŠˆê°€ ìˆìœ¼ì‹ ê°€ìš”?",
            "current_situation": "í˜„ì¬ ìƒí™©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
            "customer_segment": "ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "urgency_level": "ì´ ë¬¸ì œì˜ ê¸´ê¸‰ë„ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?",
            "available_resources": "í˜„ì¬ í™œìš© ê°€ëŠ¥í•œ ìì›(ì¸ë ¥, ì‹œìŠ¤í…œ ë“±)ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "previous_attempts": "ì´ì „ì— ì‹œë„í•´ë³¸ í•´ê²° ë°©ë²•ì´ ìˆë‚˜ìš”?",
            "customer_data": "ê³ ê° ë°ì´í„°ë‚˜ í”¼ë“œë°±ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”",
            "communication_channel": "ì£¼ë¡œ ì–´ë–¤ ì±„ë„ë¡œ ê³ ê°ê³¼ ì†Œí†µí•˜ì‹œë‚˜ìš”?",
            "timeline": "ì–¸ì œê¹Œì§€ í•´ê²°í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
            "budget": "ì˜ˆì‚° ë²”ìœ„ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”"
        }
        
        # ì§€ì‹ ê¸°ë°˜ ì´ˆê¸°í™”
        self._initialize_knowledge_base()
    
    def call_llm_api(self, model: str, prompt: str) -> str:
        """LLM API í˜¸ì¶œ í•¨ìˆ˜"""
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
                HumanMessage(content=prompt)
            ]
            
            llm = ChatOpenAI(model_name=model, temperature=0)
            raw_response = llm.invoke(messages)
            response = str(raw_response.content) if hasattr(raw_response, 'content') else str(raw_response)
            return response
            
        except Exception as e:
            logger.error(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
    def extract_info_with_llm(self, user_input: str) -> Dict[str, str]:
        """LLMì„ ì‚¬ìš©í•œ ì§€ëŠ¥ì  ì •ë³´ ì¶”ì¶œ"""
        try:
            extraction_prompt = f"""ì‚¬ìš©ìì˜ ë‹µë³€ì—ì„œ ê³ ê° ì„œë¹„ìŠ¤ ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë‹µë³€: "{user_input}"

ë‹¤ìŒ ì •ë³´ë¥¼ ì°¾ì•„ì„œ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
- business_type: ì‚¬ì—… ìœ í˜• (ì˜ˆ: ì˜¨ë¼ì¸ ì‡¼í•‘ëª°, ì¹´í˜, ë³‘ì› ë“±)
- customer_issue: ê³ ê° ë¬¸ì œ (ì˜ˆ: í´ë ˆì„, í™˜ë¶ˆìš”êµ¬, ë¶ˆë§Œ ë“±)
- desired_outcome: ì›í•˜ëŠ” ê²°ê³¼ (ì˜ˆ: ì¬ë°©ë¬¸ ìœ ë„, ë§Œì¡±ë„ í–¥ìƒ ë“±)
- current_situation: í˜„ì¬ ìƒí™© ì„¤ëª…
- customer_segment: ê³ ê°ì¸µ (ì˜ˆ: 20ëŒ€ ì—¬ì„±, ì§ì¥ì¸ ë“±)
- urgency_level: ê¸´ê¸‰ë„ (ë†’ìŒ/ë³´í†µ/ë‚®ìŒ)

ëª…í™•í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

ì‘ë‹µ ì˜ˆì‹œ:
{{"business_type": "ì˜¨ë¼ì¸ ì‡¼í•‘ëª°", "desired_outcome": "ê³ ê° ë§Œì¡±ë„ í–¥ìƒ"}}

ì‘ë‹µ:"""

            response = self.call_llm_api(model="gpt-4o-mini", prompt=extraction_prompt)
            
            # JSON ì¶”ì¶œ
            if "{" in response and "}" in response:
                json_start = response.find("{")
                json_end = response.rfind("}") + 1
                json_str = response[json_start:json_end]
                extracted = json.loads(json_str)
                logger.info(f"ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {extracted}")
                return extracted
            
            return {}
            
        except Exception as e:
            logger.error(f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
    def perform_comprehensive_analysis(self, state: ConversationState) -> str:
        """ì¢…í•©ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ ë¶„ì„ ìˆ˜í–‰"""
        try:
            collected_info = {k: v for k, v in state.collected_info.items() if v}
            
            analysis_prompt = f"""ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ êµ¬ì²´ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

**ìˆ˜ì§‘ëœ ì •ë³´:**
{json.dumps(collected_info, ensure_ascii=False, indent=2)}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ë¬¸ì œ ìƒí™© ì§„ë‹¨ ë° ì›ì¸ ë¶„ì„
2. ê³ ê° ê°ì • ìƒíƒœ ë° ë¦¬ìŠ¤í¬ í‰ê°€
3. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²° ë°©ì•ˆ (ë‹¨ê³„ë³„)
4. ê³ ê° ì¬êµ¬ë§¤/ë§Œì¡±ë„ í–¥ìƒ ì „ëµ
5. í–¥í›„ ì˜ˆë°© ë°©ì•ˆ

**ì‘ë‹µ í˜•ì‹:**
## ğŸ” ìƒí™© ë¶„ì„
- í•µì‹¬ ë¬¸ì œ: 
- ê³ ê° ê°ì • ìƒíƒœ: 
- ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: 

## ğŸ’¡ ì¦‰ì‹œ ì‹¤í–‰ ë°©ì•ˆ
### 1ë‹¨ê³„: ì´ˆê¸° ëŒ€ì‘ (24ì‹œê°„ ë‚´)
1. 
2. 

### 2ë‹¨ê³„: ë¬¸ì œ í•´ê²° (48ì‹œê°„ ë‚´)
1. 
2. 

### 3ë‹¨ê³„: ê´€ê³„ íšŒë³µ (1ì£¼ì¼ ë‚´)
1. 
2. 

## ğŸ¯ ì¬êµ¬ë§¤ ìœ ë„ ì „ëµ
- 
- 

## ğŸ›¡ï¸ í–¥í›„ ì˜ˆë°©ì±…
- 
- 

## ğŸ“‹ ê³ ê° ì‘ëŒ€ ë©”ì‹œì§€ ì˜ˆì‹œ
- ì¦‰ì‹œ ë°œì†¡ìš©: 
- í•´ê²° ì™„ë£Œ í›„:

êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            response = self.call_llm_api(model="gpt-4o", prompt=analysis_prompt)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
            state.analysis_results["comprehensive_analysis"] = response
            state.update_stage(ConversationStage.COMPLETED)
            
            return response
            
        except Exception as e:
            logger.error(f"ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def is_follow_up(self, user_input: str, last_message: str, model="gpt-4o-mini") -> bool:
        """ì´ì „ ë©”ì‹œì§€ì™€ ì—°ê²°ë˜ëŠ” í›„ì† ì§ˆë¬¸ì¸ì§€ íŒë‹¨"""
        try:
            prompt = f"""ì•„ë˜ ì‚¬ìš©ì ë°œí™”ê°€ ì´ì „ ë©”ì‹œì§€ "{last_message}"ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ” í›„ì† ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•´.
í›„ì† ì§ˆë¬¸ì´ë©´ true, ì•„ë‹ˆë©´ falseë§Œ ì¶œë ¥í•´.

ì‚¬ìš©ì ë°œí™”: "{user_input}"""
            
            response = self.call_llm_api(model=model, prompt=prompt)
            return "true" in response.lower()
            
        except Exception as e:
            logger.error(f"is_follow_up íŒë‹¨ ì‹¤íŒ¨: {e}")
            return False
    
    def _initialize_knowledge_base(self):
        """ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            vectorstore = self.vector_manager.get_vectorstore(
                collection_name=self.knowledge_collection,
                create_if_not_exists=True
            )
            
            if not vectorstore:
                logger.warning("ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            logger.info("âœ… ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def get_or_create_conversation_state(self, conversation_id: int, user_id: int) -> ConversationState:
        """ëŒ€í™” ìƒíƒœ ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        if conversation_id not in self.conversation_states:
            self.conversation_states[conversation_id] = ConversationState(conversation_id, user_id)
        return self.conversation_states[conversation_id]
    
    def classify_customer_topic_with_llm(self, user_input: str, context: str = "") -> List[str]:
        """LLMì„ í™œìš©í•œ ê³ ê° ì„œë¹„ìŠ¤ í† í”½ ë¶„ë¥˜"""
        try:
            topic_classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ê³ ê° ì„œë¹„ìŠ¤ í† í”½ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ê³ ê° ì„œë¹„ìŠ¤ í† í”½:
{chr(10).join([f"- {key}: {value}" for key, value in self.customer_topics.items()])}

{f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸: {context}" if context else ""}

ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"

ìœ„ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í† í”½ì„ ìµœëŒ€ 2ê°œê¹Œì§€ ì„ íƒí•˜ì—¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: customer_service, customer_retention

ë‹µë³€:"""

            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•í•œ ê³ ê° ê´€ë¦¬ í† í”½ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."),
                HumanMessage(content=topic_classification_prompt)
            ]
            
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
            raw_response = llm.invoke(messages)
            response = str(raw_response.content) if hasattr(raw_response, 'content') else str(raw_response)
            
            if response:
                topics = [topic.strip() for topic in response.split(',')]
                valid_topics = [topic for topic in topics if topic in self.customer_topics]
                return valid_topics[:2] if valid_topics else ["customer_service"]
            
            return ["customer_service"]
            
        except Exception as e:
            logger.error(f"LLM í† í”½ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return ["customer_service", "customer_etc"]
    
    def handle_template_request(self, user_input: str, state: ConversationState) -> str:
        """ê³ ê° ë©”ì‹œì§€ í…œí”Œë¦¿ ìš”ì²­ ì²˜ë¦¬"""
        try:
            template_type = self.extract_template_type(user_input)
            logger.info(f"ì¶”ì¶œëœ í…œí”Œë¦¿ íƒ€ì…: {template_type}")
            
            templates = get_templates_by_type(template_type)
            
            if template_type == "ê³ ê° ë§ì¶¤ ë©”ì‹œì§€" and templates:
                filtered_templates = self.filter_templates_by_query(templates, user_input)
            else:
                filtered_templates = templates
            
            if filtered_templates:
                answer_blocks = []
                for t in filtered_templates:
                    if t.get("content_type") == "html":
                        preview_url = f"http://localhost:8001/preview/{t['template_id']}"
                        answer_blocks.append(f"ğŸ“‹ **{t['title']}**\n\n[HTML ë¯¸ë¦¬ë³´ê¸°]({preview_url})")
                    else:
                        answer_blocks.append(f"ğŸ“‹ **{t['title']}**\n\n{t['content']}")
                
                answer = "\n\n---\n\n".join(answer_blocks)
                answer += f"\n\nâœ… ìœ„ í…œí”Œë¦¿ë“¤ì„ ì°¸ê³ í•˜ì—¬ ê³ ê°ì—ê²Œ ë³´ë‚¼ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”!"
                return answer
            else:
                return f"'{template_type}' ê´€ë ¨ í…œí”Œë¦¿ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
            
        except Exception as e:
            logger.error(f"í…œí”Œë¦¿ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "í…œí”Œë¦¿ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def extract_template_type(self, user_input: str) -> str:
        """í…œí”Œë¦¿ íƒ€ì… ì¶”ì¶œ"""
        template_extract_prompt = f"""ë‹¤ìŒì€ ê³ ê° ë©”ì‹œì§€ í…œí”Œë¦¿ ìœ í˜• ëª©ë¡ì…ë‹ˆë‹¤.
- ìƒì¼/ê¸°ë…ì¼
- êµ¬ë§¤ í›„ ì•ˆë‚´ (ì¶œê³  ì™„ë£Œ, ë°°ì†¡ ì‹œì‘, ë°°ì†¡ ì•ˆë‚´ ë“± í¬í•¨)
- ì¬êµ¬ë§¤ ìœ ë„
- ê³ ê° ë§ì¶¤ ë©”ì‹œì§€ (VIP, ê°€ì… ê³ ê° ë“± í¬í•¨)
- ë¦¬ë·° ìš”ì²­
- ì„¤ë¬¸ ìš”ì²­
- ì´ë²¤íŠ¸ ì•ˆë‚´
- ì˜ˆì•½
- ì¬ë°©ë¬¸
- í•´ë‹¹ì‚¬í•­ ì—†ìŒ

ì•„ë˜ ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì˜ ë§ëŠ” í…œí”Œë¦¿ ìœ í˜•ì„ í•œê¸€ë¡œ ì •í™•íˆ 1ê°œë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
ì„¤ëª… ì—†ì´ í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ì§ˆë¬¸: {user_input}
"""

        try:
            messages = [
                SystemMessage(content=template_extract_prompt),
                HumanMessage(content=user_input)
            ]
            
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
            raw_response = llm.invoke(messages)
            result = str(raw_response.content) if hasattr(raw_response, 'content') else str(raw_response)
            
            return result.strip() if result else "í•´ë‹¹ì‚¬í•­ ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"í…œí”Œë¦¿ íƒ€ì… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "í•´ë‹¹ì‚¬í•­ ì—†ìŒ"
    
    def filter_templates_by_query(self, templates: List[Dict], query: str) -> List[Dict]:
        """ì¿¼ë¦¬ì— ë”°ë¥¸ í…œí”Œë¦¿ í•„í„°ë§"""
        query_lower = query.lower()
        filtered = []
        
        for t in templates:
            title = t.get('title', '')
            title_lower = title.lower()
            
            if ('vip' in query_lower or 'ë‹¨ê³¨' in query_lower) and ('vip' in title_lower or 'ë‹¨ê³¨' in title_lower):
                filtered.append(t)
            elif ('íœ´ë©´' in query_lower or 'ì¥ê¸°ë¯¸êµ¬ë§¤' in query_lower) and 'íœ´ë©´' in title:
                filtered.append(t)
            elif ('ê°€ì…' in query_lower or 'íšŒì›ê°€ì…' in query_lower) and ('ê°€ì…' in title_lower or 'íšŒì›ê°€ì…' in title_lower):
                filtered.append(t)
            elif ('ìµœê·¼ êµ¬ë§¤' in query_lower or 'ìµœê·¼êµ¬ë§¤' in query_lower) and ('ìµœê·¼ êµ¬ë§¤' in title_lower or 'ìµœê·¼êµ¬ë§¤' in title_lower):
                filtered.append(t)
        
        return filtered if filtered else templates
    
    def _determine_conversation_mode_with_history(self, user_input: str, user_id: int, conversation_id: Optional[int] = None) -> bool:
        """íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•œ ì‹±ê¸€í„´/ë©€í‹°í„´ ëª¨ë“œ ìë™ íŒë‹¨"""
        try:
            # ğŸ”¥ ìˆ˜ì •: ê¸°ì¡´ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë©€í‹°í„´ ìœ ì§€
            if conversation_id is not None:
                try:
                    with get_session_context() as db:
                        recent_messages = get_recent_messages(db, conversation_id, limit=1)
                        if recent_messages:
                            logger.info("ê¸°ì¡´ ëŒ€í™” ì¡´ì¬ - ë©€í‹°í„´ ëª¨ë“œ ìœ ì§€")
                            return False  # ë©€í‹°í„´ ìœ ì§€
                except:
                    pass
            
            # ì²« ëŒ€í™”ì¼ ë•Œë§Œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
            return self._determine_conversation_mode_by_keywords(user_input)
            
        except Exception as e:
            logger.error(f"íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ëŒ€í™” ëª¨ë“œ íŒë‹¨ ì‹¤íŒ¨: {e}")
            return True
    
    def _determine_conversation_mode_by_keywords(self, user_input: str) -> bool:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì‹±ê¸€í„´/ë©€í‹°í„´ ëª¨ë“œ íŒë‹¨ (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            # ğŸ”¥ ìˆ˜ì •: í…œí”Œë¦¿ ìš”ì²­ì€ ë¬´ì¡°ê±´ ì‹±ê¸€í„´
            if any(keyword in user_input for keyword in ["í…œí”Œë¦¿", "ë©”ì‹œì§€", "ë¬¸êµ¬", "ì•Œë¦¼", "ì˜ˆì‹œ"]):
                return True
            
            # ë©€í‹°í„´ í‚¤ì›Œë“œ (ìƒë‹´/ë¶„ì„ í•„ìš”)
            multi_turn_keywords = [
                "ìƒë‹´", "ë„ì›€", "í•´ê²°", "ë¶„ì„", "ê³„íš", "ì „ëµ",
                "ë‹¨ê³„ë³„", "ìì„¸íˆ", "ì²´ê³„ì ", "ì»¨ì„¤íŒ…",
                "ê¸´ê¸‰", "ë¬¸ì œ", "ê°œì„ ", "ì „ë¬¸ì ", "ì¡°ì–¸",
                "í´ë ˆì„", "ê³ ê°", "ì‘ëŒ€", "ì²˜ë¦¬", "ê´€ë¦¬",
                "ë¶ˆë§Œ", "í™˜ë¶ˆ", "ì¬ë°©ë¬¸", "ë§Œì¡±ë„", "ê´€ê³„",
                "ì–´ë–»ê²Œ ì‘ëŒ€", "ì–´ë–»ê²Œ ì²˜ë¦¬", "ì–´ë–»ê²Œ í•´ê²°",
                "ì–´ë–»ê²Œ ê´€ë¦¬", "ì–´ë–»ê²Œ ê°œì„ ", "ì–´ë–¤ ë°©ë²•"
            ]
            
            user_lower = user_input.lower()
            
            # ë©€í‹°í„´ í‚¤ì›Œë“œ ì²´í¬
            for keyword in multi_turn_keywords:
                if keyword in user_lower:
                    return False  # ë©€í‹°í„´
            
            # ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ íŒë‹¨
            if len(user_input) < 20 and user_input.count('?') <= 1:
                return True  # ì§§ê³  ë‹¨ìˆœí•œ ì§ˆë¬¸ì€ ì‹±ê¸€í„´
            
            # ê¸°ë³¸ê°’: ë³µì¡í•œ ì§ˆë¬¸ì€ ë©€í‹°í„´
            return False
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ê¸°ë°˜ ëŒ€í™” ëª¨ë“œ íŒë‹¨ ì‹¤íŒ¨: {e}")
            return False
    
    def _process_single_turn_query(self, user_input: str, user_id: int) -> Dict[str, Any]:
        """ì‹±ê¸€í„´ ëŒ€í™” ì²˜ë¦¬"""
        try:
            if any(keyword in user_input for keyword in ["í…œí”Œë¦¿", "ë©”ì‹œì§€", "ë¬¸êµ¬", "ì•Œë¦¼"]):
                response_content = self._handle_single_turn_template_request(user_input, user_id)
            else:
                response_content = self._handle_single_turn_general_query(user_input, user_id)
            
            return create_success_response({
                "answer": response_content,
                "agent_type": "customer_service",
                "mode": "single_turn",
                "timestamp": get_current_timestamp()
            })
            
        except Exception as e:
            logger.error(f"ì‹±ê¸€í„´ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response(
                error_message=f"ì‹±ê¸€í„´ ìƒë‹´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                error_code="SINGLE_TURN_ERROR"
            )
    
    def _handle_single_turn_template_request(self, user_input: str) -> str:
        """ì‹±ê¸€í„´ í…œí”Œë¦¿ ìš”ì²­ ì²˜ë¦¬"""
        try:
            template_type = self.extract_template_type(user_input)
            templates = get_templates_by_type(template_type)
            
            if template_type == "ê³ ê° ë§ì¶¤ ë©”ì‹œì§€" and templates:
                filtered_templates = self.filter_templates_by_query(templates, user_input)
            else:
                filtered_templates = templates
            
            if filtered_templates:
                answer_blocks = []
                for t in filtered_templates:
                    if t.get("content_type") == "html":
                        preview_url = f"http://localhost:8001/preview/{t['template_id']}"
                        answer_blocks.append(f"ğŸ“‹ **{t['title']}**\n\n[HTML ë¯¸ë¦¬ë³´ê¸°]({preview_url})")
                    else:
                        answer_blocks.append(f"ğŸ“‹ **{t['title']}**\n\n{t['content']}")
                
                answer = "\n\n---\n\n".join(answer_blocks)
                answer += f"\n\nâœ… ìœ„ í…œí”Œë¦¿ë“¤ì„ ì°¸ê³ í•˜ì—¬ ê³ ê°ì—ê²Œ ë³´ë‚¼ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”!"
                return answer
            else:
                return f"'{template_type}' ê´€ë ¨ í…œí”Œë¦¿ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
            
        except Exception as e:
            logger.error(f"ì‹±ê¸€í„´ í…œí”Œë¦¿ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return "í…œí”Œë¦¿ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    def get_user_persona_info(self, user_id: int) -> dict:
        """ì‚¬ìš©ìì˜ í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¡°íšŒ"""
        try:
            with get_session_context() as db:
                persona_info = get_user_persona_info(db, user_id)
            
            logger.info(f"í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {persona_info}")
            return persona_info
            
        except Exception as e:
            logger.error(f"í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
        
    def _handle_single_turn_general_query(self, user_input: str, user_id: int) -> str:
        """ì‹±ê¸€í„´ ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ (í˜ë¥´ì†Œë‚˜ ì ìš©)"""
        try:
            # ğŸ”¥ í˜ë¥´ì†Œë‚˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            persona_info = self.get_user_persona_info(user_id)
            
            topics = self.classify_customer_topic_with_llm(user_input)
            primary_topic = topics[0] if topics else "customer_service"
            
            knowledge_texts = self.get_relevant_knowledge(user_input, topics)
            
            # ğŸ”¥ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            persona_context = ""
            if persona_info:
                business_type = persona_info.get('business_type', '')
                nickname = persona_info.get('nickname', 'ì‚¬ì¥ë‹˜')
                experience = persona_info.get('experience', 0)
                exp_level = "ì´ˆë³´ì" if experience == 0 else "ê²½í—˜ì"


                persona_context = ""
                if persona_info:
                    business_type = persona_info.get('business_type', '')
                    experience = persona_info.get('experience', 0)
                    
                    persona_context = f"""
        ì‚¬ìš©ìëŠ” {business_type} ì—…ì¢…ì—ì„œ ì¼í•˜ë©°, {'ê³ ê° ê´€ë¦¬ê°€ ì²˜ìŒì¸' if experience == 0 else 'ê³ ê° ê´€ë¦¬ ê²½í—˜ì´ ìˆëŠ”'} ìƒí™©ì…ë‹ˆë‹¤.
        ì´ ì—…ì¢… íŠ¹ì„±ì— ë§ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í•´ì£¼ë˜, ì—…ì¢…ì„ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•´ì£¼ì„¸ìš”.
        {'ê¸°ë³¸ ê°œë…ë¶€í„° ì‰½ê²Œ ì„¤ëª…í•˜ë©°' if experience == 0 else 'ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ê³ ê¸‰ íŒì„ ì œê³µí•˜ê³ '} ì‹¤ì œ ìƒí™©ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.
        """
            
            general_prompt = f"""ë‹¹ì‹ ì€ ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

    {persona_context}

    ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"
    ì£¼ìš” í† í”½: {primary_topic}

    {"ê´€ë ¨ ì „ë¬¸ ì§€ì‹:" + chr(10) + chr(10).join(knowledge_texts) if knowledge_texts else ""}

    ğŸ’¼ ì‘ë‹µ ì§€ì¹¨:
    1. ì—…ì¢…ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì¡°ì–¸
    2. êµ¬ì²´ì ì´ê³  ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…
    3. ì¹œê·¼í•˜ê³  ê³µê°ì ì¸ í†¤(ê³¼ë„í•œ ì¸ì‚¬ë‚˜ ì¶•í•˜ëŠ” ìƒëµ)
    4. ì‹¤ì œ ìƒí™© ì˜ˆì‹œ í¬í•¨
    5. ê²½í—˜ ìˆ˜ì¤€ì— ë§ëŠ” ì„¤ëª…

    ì‘ë‹µ:"""
            
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ê³ ê° ì„œë¹„ìŠ¤ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ì‚¬ìš©ìì˜ ì—…ì¢…ê³¼ ê²½í—˜ ìˆ˜ì¤€ì— ë§ëŠ” ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤."),
                HumanMessage(content=general_prompt)
            ]
            
            llm = ChatOpenAI(model_name="gpt-4o", temperature=0.8)  # ë” ì°½ì˜ì ìœ¼ë¡œ
            raw_response = llm.invoke(messages)
            response = str(raw_response.content) if hasattr(raw_response, 'content') else str(raw_response)
            
            return response if response else "ë” êµ¬ì²´ì ì¸ ìƒí™©ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë§ì¶¤í˜• ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”."
            
        except Exception as e:
            logger.error(f"ì‹±ê¸€í„´ ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
    def _process_multi_turn_query(self, user_input: str, user_id: int, conversation_id: Optional[int] = None) -> Dict[str, Any]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            # ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬
            session_info = get_or_create_conversation_session(user_id, conversation_id)
            conversation_id = session_info["conversation_id"]
            
            # ëŒ€í™” ìƒíƒœ ì¡°íšŒ/ìƒì„±
            state = self.get_or_create_conversation_state(conversation_id, user_id)
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            with get_session_context() as db:
                create_message(db, conversation_id, "user", "customer_service", user_input)
            
            # í…œí”Œë¦¿ ìš”ì²­ ì²´í¬
            if any(keyword in user_input for keyword in ["í…œí”Œë¦¿", "ë©”ì‹œì§€", "ë¬¸êµ¬", "ì•Œë¦¼"]):
                response_content = self.handle_template_request(user_input, state)
            else:
                # ğŸ”¥ ìˆ˜ì •: í˜„ì¬ ë‹¨ê³„ì— ë”°ë¥¸ ì²˜ë¦¬
                if state.stage == ConversationStage.INITIAL:
                    # ì²« ì§ˆë¬¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    extracted_info = self.extract_info_with_llm(user_input)
                    
                    # ì¶”ì¶œëœ ì •ë³´ ì €ì¥
                    for key, value in extracted_info.items():
                        if key in state.collected_info:
                            state.add_collected_info(key, value)
                    
                    # ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë¶„ì„
                    if state.is_information_complete():
                        logger.info("ì²« ì§ˆë¬¸ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ í™•ë³´ - ì¦‰ì‹œ ë¶„ì„")
                        state.update_stage(ConversationStage.ANALYSIS)
                        analysis_result = self.perform_comprehensive_analysis(state)
                        
                        collected_summary = self._create_collected_info_summary(state)
                        response_content = f"""

{analysis_result}"""
                    else:
                        # ì¶”ê°€ ì •ë³´ í•„ìš”
                        state.update_stage(ConversationStage.INFORMATION_GATHERING)
                        response_content = f"""ì•ˆë…•í•˜ì„¸ìš”! ê³ ê° ì„œë¹„ìŠ¤ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸ˜Š  

ë³´ë‹¤ ì •í™•í•˜ê³  ìƒí™©ì— ë§ëŠ” í•´ê²°ì±…ì„ ë“œë¦¬ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì •ë³´ê°€ ë” í•„ìš”í•´ìš”.
  
**{self._get_next_question(state)}**

ë„ˆë¬´ ì–´ë µê²Œ ìƒê°í•˜ì§€ ë§ˆì‹œê³ , í¸í•˜ê²Œ ë‹µí•´ì£¼ì‹œë©´ ë¼ìš”. í•¨ê»˜ ì°¨ê·¼ì°¨ê·¼ í’€ì–´ë‚˜ê°€ ë³¼ê²Œìš”!"""
                
                elif state.stage == ConversationStage.INFORMATION_GATHERING:
                    # ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„
                    response_content = self.handle_information_gathering(user_input, state)
                
                elif state.stage == ConversationStage.ANALYSIS:
                    # ë¶„ì„ ì™„ë£Œ í›„ ì¶”ê°€ ì§ˆë¬¸
                    response_content = "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ìˆ˜ì •ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"
                
                else:
                    response_content = "ìƒë‹´ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"
            
            # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            insert_message_raw(
                conversation_id=conversation_id,
                sender_type="agent",
                agent_type="customer_service",
                content=response_content
            )
            
            # í‘œì¤€ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            try:
                from shared_modules.standard_responses import create_customer_response
                return create_customer_response(
                    conversation_id=conversation_id,
                    answer=response_content,
                    topics=getattr(state.analysis_results, 'primary_topics', []),
                    sources=f"ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ (ë‹¨ê³„: {state.stage.value})",
                    conversation_stage=state.stage.value,
                    completion_rate=state.get_completion_rate(),
                    collected_info=state.collected_info,
                    multiturn_flow=True
                )
            except ImportError:
                # ë°±ì—…ìš© í‘œì¤€ ì‘ë‹µ
                return create_success_response({
                    "conversation_id": conversation_id,
                    "answer": response_content,
                    "agent_type": "customer_service",
                    "stage": state.stage.value,
                    "completion_rate": state.get_completion_rate(),
                    "timestamp": get_current_timestamp()
                })
                
        except Exception as e:
            logger.error(f"ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response(f"ë©€í‹°í„´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", "MULTITURN_ERROR")
    
    def _get_next_question(self, state: ConversationState) -> str:
        """ë‹¤ìŒ ì§ˆë¬¸ ì„ íƒ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        for field, question in self.info_gathering_questions.items():
            if not state.collected_info.get(field):
                return question
        return "ì¶”ê°€ë¡œ ì•Œë ¤ì£¼ì‹¤ ë‚´ìš©ì´ ìˆë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”."
    
    def _create_collected_info_summary(self, state: ConversationState) -> str:
        """ìˆ˜ì§‘ëœ ì •ë³´ ìš”ì•½ ìƒì„±"""
        collected_summary = []
        for field, value in state.collected_info.items():
            if value:
                field_name = self.info_gathering_questions.get(field, field)
                collected_summary.append(f"âœ“ {field_name}: {value}")
        
        if collected_summary:
            return f"""**ìˆ˜ì§‘ëœ ì •ë³´ ({len(collected_summary)}ê°œ):**
{chr(10).join(collected_summary)}"""
        else:
            return "**ìˆ˜ì§‘ëœ ì •ë³´:** ì•„ì§ ì—†ìŒ"
    
    def handle_information_gathering(self, user_input: str, state: ConversationState) -> str:
        """ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ ì²˜ë¦¬ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        try:
            # LLMìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
            extracted_info = self.extract_info_with_llm(user_input)
            
            # ì¶”ì¶œëœ ì •ë³´ë¥¼ ìƒíƒœì— ì—…ë°ì´íŠ¸
            for key, value in extracted_info.items():
                if key in state.collected_info:
                    state.add_collected_info(key, value)
                    logger.info(f"ì •ë³´ ì—…ë°ì´íŠ¸: {key} = {value}")
            
            # ğŸ”¥ ìˆ˜ì •: ì™„ë£Œ ì¡°ê±´ ì²´í¬
            filled_count = len([v for v in state.collected_info.values() if v])
            
            if state.is_information_complete() or filled_count >= 3:
                logger.info(f"ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ - ë¶„ì„ ì‹œì‘ (í•„ë“œ: {filled_count}ê°œ)")
                
                # ë¶„ì„ ë‹¨ê³„ë¡œ ì „í™˜í•˜ê³  ì¦‰ì‹œ ë¶„ì„ ìˆ˜í–‰
                state.update_stage(ConversationStage.ANALYSIS)
                analysis_result = self.perform_comprehensive_analysis(state)
                
                collected_summary = self._create_collected_info_summary(state)
                
                return f"""


{analysis_result}"""
            
            # ì•„ì§ ë” ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
            else:
                next_question = self._get_next_question(state)
                collected_summary = self._create_collected_info_summary(state)
                
                return f"""

{next_question}

ë” ì •í™•í•œ í•´ê²°ì±…ì„ ìœ„í•´ ìœ„ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”!"""
                
        except Exception as e:
            logger.error(f"ì •ë³´ ìˆ˜ì§‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def process_user_query(
        self, 
        user_input: str, 
        user_id: int, 
        conversation_id: Optional[int] = None,
        single_turn: Optional[bool] = None
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ìë™ ë©€í‹°í„´/ì‹±ê¸€í„´ ëŒ€í™” ì§€ì›"""
        
        try:
            single_turn=True
            # 1. ëŒ€í™” ëª¨ë“œ ìë™ íŒë‹¨ (single_turnì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°)
            if single_turn is None:
                single_turn = self._determine_conversation_mode_with_history(user_input, user_id, conversation_id)
                
            logger.info(f"{'ì‹±ê¸€í„´' if single_turn else 'ë©€í‹°í„´'} ëª¨ë“œë¡œ ê³ ê° ì„œë¹„ìŠ¤ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {user_input[:50]}...")
            
            # 2. ì‹±ê¸€í„´ ëª¨ë“œ ì²˜ë¦¬
            if single_turn:
                return self._process_single_turn_query(user_input, user_id)
            
            # 3. ë©€í‹°í„´ ëª¨ë“œ ì²˜ë¦¬
            return self._process_multi_turn_query(user_input, user_id, conversation_id)
            
        except Exception as e:
            logger.error(f"{'ì‹±ê¸€í„´' if single_turn else 'ë©€í‹°í„´'} ê³ ê° ì„œë¹„ìŠ¤ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response(
                error_message=f"ê³ ê° ì„œë¹„ìŠ¤ ìƒë‹´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                error_code="CUSTOMER_SERVICE_ERROR"
            )
    
    def get_agent_status(self) -> Dict[str, Any]:
        """ê³ ê° ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ìƒíƒœ ë°˜í™˜"""
        return {
            "agent_type": "customer_service",
            "version": "3.1.0",
            "conversation_system": "smart_multiturn",
            "stages": [stage.value for stage in ConversationStage],
            "active_conversations": len(self.conversation_states),
            "conversation_stages": {
                conv_id: state.stage.value 
                for conv_id, state in self.conversation_states.items()
            },
            "supported_features": [
                "ê³ ê° ë©”ì‹œì§€ í…œí”Œë¦¿",
                "ì§€ëŠ¥ì  ë©€í‹°í„´/ì‹±ê¸€í„´ ëª¨ë“œ",
                "LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ",
                "ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰",
                "ê°„ì†Œí™”ëœ ëŒ€í™” íë¦„"
            ]
        }

    def get_relevant_knowledge(self, query: str, topics: List[str] = None) -> List[str]:
        """ì‹¤ì œ ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰"""
        try:
            if not hasattr(self.vector_manager, 'search_documents'):
                return []
                
            search_results = self.vector_manager.search_documents(
                query=query,
                collection_name=self.knowledge_collection,
                k=5
            )
            
            filtered_results = []
            for doc in search_results:
                if doc.metadata.get('type') != 'prompt_template':
                    filtered_results.append(doc)
            
            knowledge_texts = []
            for doc in filtered_results[:3]:
                knowledge_area = doc.metadata.get('knowledge_area', 'ì¼ë°˜')
                content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                knowledge_texts.append(f"[{knowledge_area}]\n{content}")
            
            return knowledge_texts
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []