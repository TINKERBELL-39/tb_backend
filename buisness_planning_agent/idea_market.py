import mcp
from mcp.client.streamable_http import streamablehttp_client
from dateutil.relativedelta import relativedelta
from datetime import datetime
import asyncio
import base64
import json
import re

from dotenv import load_dotenv
import os


# simithery_api_key
load_dotenv('../unified_agent_system/.env') # ë””ë²„ê¹…ìœ¼ë¡œ í‚¬ ë•ŒëŠ” ./unfied
smithery_api_key=os.getenv("SMITHERY_API_KEY")
google_api_key=os.getenv("GOOGLE_API_KEY")

print(f"smithery_api_key : {smithery_api_key}")
# smithery_mcp_url
app_store_url = (
    "https://server.smithery.ai/@JiantaoFu/appinsightmcp/mcp"
    f"?api_key={smithery_api_key}&profile=blank-tahr-7BPYtb"
)
amazon_url = (
    f"https://server.smithery.ai/@SiliconValleyInsight/amazon-product-search/mcp"
    f"?api_key={smithery_api_key}&profile=blank-tahr-7BPYtb"
)

bright_data_url = (
    f"https://server.smithery.ai/@luminati-io/brightdata-mcp/mcp"
    f"?api_key={smithery_api_key}&profile=blank-tahr-7BPYtb"
)

youtube_config = {
    "youtubeTranscriptLang": "ko",
    "youtubeApiKey": os.getenv("YOUTUBE_API_KEY"),
}
youtube_config_b64 = base64.b64encode(json.dumps(youtube_config).encode()).decode()

youtube_url = f"https://server.smithery.ai/@icraft2170/youtube-data-mcp-server/mcp?config={youtube_config_b64}&api_key={smithery_api_key}&profile=blank-tahr-7BPYtb"


##########################################################################

async def get_trending_new_app(url):
    answer = ["[í•œêµ­ App Storeì—ì„œ ì‹ ê·œ ì¸ê¸° ì•±] "]
    async with streamablehttp_client(url) as (read_stream, write_stream, _):
        async with mcp.ClientSession(read_stream, write_stream) as session:
            await session.initialize()

            #ğŸ‡°ğŸ‡· ì•±ìŠ¤í† ì–´ ì‹ ê·œì¸ê¸°ì•±
            params = {
                "collection": "newapplications",
                "country": "kr",
                #"num": max_results,
            }
               
         
            result = await session.call_tool("app-store-list", params)
            apps = json.loads(result.content[0].text)
                
            if isinstance(apps, dict):
                apps = apps.get("apps", [])
            for i, app in enumerate(apps, 1):
                answer.append(f"{i}. {app.get('title')} | {app.get('genre')}")
    return "\n".join(answer)      


async def get_trending_amazon_products(url=amazon_url, max_results=20):
    answer = ["[ì•„ë§ˆì¡´ íŠ¸ë Œë”© ìƒí’ˆ] "]
    async with streamablehttp_client(url) as (read_stream, write_stream, _):
        async with mcp.ClientSession(read_stream, write_stream) as session:
            await session.initialize()
            search_input = {
                "query": f"ì•„ë§ˆì¡´ top {max_results} íŠ¸ë Œë”© ìƒí’ˆì´ ë­ì•¼?",
                "maxResults": max_results,
            }
            result = await session.call_tool("find_products_to_buy", search_input)
            result_text = result.content[0].text
            
            # ì •ì œ
            products = parse_amazon_plain_text_products(result_text, max_results=max_results)
            for idx, item in enumerate(products, 1):
                title = item.get("title", "ì œëª©ì—†ìŒ")
                price = item.get("price", "(ê°€ê²©ì •ë³´ì—†ìŒ)")
               
                answer.append(
                    f"{idx}. {title} - ê°€ê²©: {price}\n"
                )
    return "\n".join(answer)

async def get_trending_youtube_videos(
    youtube_url: str,
    region_code: str = "KR",
    max_results: int = 15,
    category_id: int | None = None
) -> str:
  
    async with streamablehttp_client(youtube_url) as (read_stream, write_stream, _):
        async with mcp.ClientSession(read_stream, write_stream) as session:
            await session.initialize()

            trending_input = {
                "regionCode": region_code,
                "maxResults": max_results,
            }
            if category_id is not None:
                trending_input["categoryId"] = str(category_id)

            trending_result = await session.call_tool("getTrendingVideos", trending_input)
            trending_json = trending_result.content[0].text
            trending_videos = json.loads(trending_json)

            video_ids = [item["id"] for item in trending_videos]
            details_result = await session.call_tool("getVideoDetails", {"videoIds": video_ids})
            details_json = details_result.content[0].text
            details_dict = json.loads(details_json)

            output_lines = []
            output_lines.append(f"ğŸ”¥ ìœ íŠœë¸Œ ì¸ê¸° ì˜ìƒ TOP {max_results} ìƒì„¸\n")
            for rank, video_id in enumerate(video_ids, 1):
                detail = details_dict.get(video_id)
                if not detail:
                    output_lines.append(f"top{rank}. ì •ë³´ ì—†ìŒ\n")
                    continue

                snippet    = detail.get("snippet", {})
                statistics = detail.get("statistics", {})
                content_details = detail.get("contentDetails", {})

                tags       = snippet.get("tags", [])
                title      = snippet.get("title")
                channel    = snippet.get("channelTitle")
                duration_iso = content_details.get("duration") or ""
                duration_str = parse_duration(duration_iso)
                description= snippet.get("description", "").strip()
                view       = statistics.get("viewCount")
                like       = statistics.get("likeCount")
                comment    = statistics.get("commentCount")
                published  = snippet.get("publishedAt")

                output_lines.append(
                    f"top{rank}. [{title}]\n"
                    f"ì±„ë„: {channel}\n"
                    f"ì˜ìƒë§í¬: https://youtube.com/watch?v={video_id}\n"
                    f"ì˜ìƒê¸¸ì´: {duration_str}\n"
                    f"íƒœê·¸: {', '.join(tags) if tags else '(ì—†ìŒ)'}\n"
                    f"ì„¤ëª…: {description}\n"
                    f"ì¡°íšŒìˆ˜: {view}\n"
                    f"ì¢‹ì•„ìš”ìˆ˜: {like}\n"
                    f"ëŒ“ê¸€ìˆ˜: {comment}\n"
                    f"ì—…ë¡œë“œì¼: {published}\n"
                )

            return "\n".join(output_lines)


def parse_amazon_plain_text_products(raw_text, max_results=5):
    lines = raw_text.splitlines()
    products = []
    curr = {}
    for line in lines:
        line = line.strip()
        m = re.match(r"^(\d+)\.\s+(.*)", line)
        if m:
            if curr:
                products.append(curr)
                if len(products) >= max_results:
                    break
                curr = {}
            curr['title'] = m.group(2)
        elif line.startswith("Price:"):
            curr['price'] = line.replace("Price:", "").strip()
        elif line.startswith("Image:"):
            curr['image'] = line.replace("Image:", "").strip()
        elif line.startswith("Product Link:"):
            curr['product_url'] = line.replace("Product Link:", "").strip()
        elif line and not line.startswith("Found "):
            if 'short_desc' not in curr:
                curr['short_desc'] = line
    if curr and len(products) < max_results:
        products.append(curr)
    return products[:max_results]

YOUTUBE_CATEGORY_KR_DICT = {
    1:  "ì˜í™” ë° ì• ë‹ˆë©”ì´ì…˜",
    2:  "ìë™ì°¨ ë° ì°¨ëŸ‰",
    10: "ìŒì•…",
    15: "ë°˜ë ¤ë™ë¬¼ ë° ë™ë¬¼",
    17: "ìŠ¤í¬ì¸ ",
    19: "ì—¬í–‰ ë° ì´ë²¤íŠ¸",
    20: "ê²Œì„",
    22: "ì¸ë¬¼ & ë¸”ë¡œê·¸",
    23: "ì½”ë¯¸ë””",
    24: "ì—”í„°í…Œì¸ë¨¼íŠ¸",
    25: "ë‰´ìŠ¤ ë° ì •ì¹˜",
    26: "Howto & Style (ìš”ë¦¬/ìŠ¤íƒ€ì¼/ë¨¹ë°© ë“±)",
    27: "êµìœ¡",
    28: "ê³¼í•™ ë° ê¸°ìˆ ",
}

def parse_duration(iso_duration):
    """ISO8601 PT#H#M#S â†’ 'Më¶„ Sì´ˆ' ë˜ëŠ” 'Hì‹œê°„ Më¶„ Sì´ˆ' ë³€í™˜"""
    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", iso_duration)
    if not match:
        return "?"
    h = int(match.group(1) or 0)
    m = int(match.group(2) or 0)
    s = int(match.group(3) or 0)
    if h:
        return f"{h}ì‹œê°„ {m}ë¶„ {s}ì´ˆ"
    return f"{m}ë¶„ {s}ì´ˆ" if m or s else "0ì´ˆ"


today=datetime.now()
min_date = today - relativedelta(years=1,months=6)

async def get_common(url,query, max_results=2):
    print("[DEBUG] get_common ì§„ì…")
    try:
        answer = []
        async with streamablehttp_client(url) as (read_stream, write_stream, _):
            async with mcp.ClientSession(read_stream, write_stream) as session:
                await session.initialize()
                
                params = {"query": query, "engine": "google", "min_date": min_date}
                search_result = await session.call_tool("search_engine", params)
                # try:
                #     text = search_result.content[0].text
                # except Exception as e:
                #     print(f"[ERROR] search_engine ê²°ê³¼ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                #     return "ê²€ìƒ‰ ì‹¤íŒ¨"
                if not hasattr(search_result, "content") or not search_result.content:
                    print("[ERROR] search_engine ì‘ë‹µì´ ë¹„ì •ìƒ")
                    return "ê²€ìƒ‰ ì‹¤íŒ¨"
                text = getattr(search_result.content[0], "text", "")
                
                url_pattern = re.compile(r"\(http[^)]+\)")
                url_candidates = url_pattern.findall(text)
                links = [s[1:-1] for s in url_candidates]

                try:
                    links = links[6:] # êµ¬ê¸€ ë¶€ê°€ë§í¬ ì œì™¸
                    print(links)
                    if len(links) < 1:
                        raise ValueError
                except Exception:
                    return "\n[!] URL ì¶”ì¶œì„ ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\nê²€ìƒ‰ì–´ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¾¸ê±°ë‚˜, ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„í•´ ì£¼ì„¸ìš”."

                # 2. ë³¸ë¬¸ ìˆ˜ì§‘ (ì—ëŸ¬ ì—†ëŠ” 2ê°œê¹Œì§€)
                collected_texts = []
                count = 0
                for url_ in links:
                    if count >= max_results:
                        break
                    try:
                        scrape_result = await session.call_tool("scrape_as_markdown", {"url": url_})
                        if not getattr(scrape_result, "isError", False):
                            content = (scrape_result.content[0].text or '').strip()
                            if len(content)<=100:
                                continue
                            collected_texts.append(content)
                            count += 1
                    except Exception as e:
                        continue

                if not collected_texts:
                    return "[!] ë³¸ë¬¸ ì¶”ì¶œì— ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”."

                all_content = "â¯â¯ ë‹¤ìŒ ìë£Œ â¯â¯".join(collected_texts)
                all_content = re.sub(r'!\[.*?\]\([^)]+\)', '', all_content)
                all_content = re.sub(r'\(http[^)]+\)', '', all_content)
                all_content = re.sub(r"\{.*?\}", "", all_content) # { } ìˆìœ¼ë©´ ì—ëŸ¬ë‚¨
                all_content = all_content.replace("{", "").replace("}", "")
                all_content = re.sub(r'\[[^\[\]]*\]', '', all_content)
                all_content = re.sub(r'\n+', ' ', all_content)
                answer.append(all_content)
        return "\n".join(answer)
    
    except Exception as e:
        print("[!!EXCEPTION in get_common!!]", type(e), e)
        import traceback; traceback.print_exc()
        raise

############### ì°½ì—… ì•„ì´í…œ ì¶”ì²œ ################
################ ë¶„ë¥˜ #################
async def get_persona_trend(persona: str, query: str):
    """
    ì‚¬ìš©ì personaì— ë§ì¶° ìµœì ì˜ íŠ¸ë Œë“œ/ì‹œì¥/ê²½ìŸì‚¬ ë°ì´í„° ì·¨ë“ í•¨ìˆ˜
    - persona: creator, beautyshop, e_commerce, developer, common
    - return: í…ìŠ¤íŠ¸(string)
    """
    # e_commerceëŠ” ì•„ë§ˆì¡´ íŠ¸ë Œë”© ìƒí’ˆ
    if persona == "ecommerce":
        trend=await get_trending_amazon_products(amazon_url)
        mcp_source="smithery_ai/amazon-product-search"
        return trend,mcp_source
    
    # developerëŠ” í•œêµ­ ì•±ìŠ¤í† ì–´ ì‹ ê·œ ì¸ê¸° ì•±
    elif persona == "developer":
        smithery_api_key = "056f88d0-aa2e-4ea9-8f2d-382ba74dcb07"
        app_store_url = f"https://server.smithery.ai/@JiantaoFu/appinsightmcp/mcp?api_key={smithery_api_key}&profile=realistic-possum-fgq4Y7"
        trend=await get_trending_new_app(app_store_url)
        mcp_source="smithery_ai/appinsight"
        return trend,mcp_source
    
    # creatorëŠ” ìœ íŠœë¸Œ ì¸ê¸° íŠ¸ë Œë“œ
    elif persona == "creator":
        smithery_api_key = "056f88d0-aa2e-4ea9-8f2d-382ba74dcb07"
        youtube_url = f"https://server.smithery.ai/@icraft2170/youtube-data-mcp-server/mcp?api_key={smithery_api_key}&profile=realistic-possum-fgq4Y7"
        trend= await get_trending_youtube_videos(youtube_url)
        mcp_source = "smithery_ai/youtube-data-mcp-server"
        return trend,mcp_source
    
    # ë‚˜ë¨¸ì§€(ì˜ˆì™¸)ëŠ” commonê³¼ ë™ì‘ ë™ì¼
    else:
        trend=await get_common(bright_data_url, query)
        mcp_source="smithery_ai/brightdata-search"
        return trend, mcp_source

################################################################################################################################################################
# idea_validation 

def clean_korean_query(text: str) -> str:
    # 1. í•œê¸€ ì¡°ì‚¬ ì œê±°
    JOSA = r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ë„|ì—|ì—ì„œ|ê¹Œì§€|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì™€ì˜|ë‘|í•˜ê³ |ì—ê²Œ|ê»˜|ë³´ë‹¤|ë§ˆì €|ì¡°ì°¨|ì²˜ëŸ¼|ë§Œí¼|ë¿|ë°–ì—|ì´ë¼ë„|ì´ë‚˜|ë©°|ë“ ì§€|ë¼ë„|ì´ë‚˜ë§ˆ|ê»˜ì„œ)'
    text = re.sub(rf'\b(\w+){JOSA}\b', r'\1', text)
    text = re.sub(rf'\b{JOSA}\b', '', text)

    # 2. ëª…ë ¹/ë¶€íƒí˜• íŒ¨í„´ ëª¨ë‘ ì œê±°
    pattern = r'''
        (ì¢€)|
        (ì•Œë ¤[\s]*ì¤˜)|
        (ì•Œë ¤[\s]*ì£¼ì„¸ìš”)|
        (ì•Œë ¤[\s]*ì£¼[ì‹œ]*ê² ì–´ìš”?)|
        (ì•Œë ¤[\s]*ì£¼[ì‹¤]*ë˜ìš”?)|
        (ì•Œë ¤[\s]*ì£¼[ì‹¤]*ìˆ˜[\s]*ìˆë‚˜ìš”?)|
        (ì•Œë ¤[\s]*ì£¼ì…¨ìœ¼ë©´[\s]*ì¢‹ê² ì–´ìš”?)|
        (ì•Œë ¤[\s]*ì£¼ì‹­ì‹œì˜¤)|
        (ì•Œë ¤[\s]*ì£¼ì…”ìš”)|
        (ì•Œë ¤[\s]*ì£¼ê³ [\s]*ì‹¶ì–´ìš”)|
        (í•´[\s]*ì¤˜)|
        (í•´[\s]*ì£¼ì„¸ìš”)|
        (í•´[ì£¼ì‹œ]*ê² ì–´ìš”?)|
        (í•´[ì£¼ì‹œ]*ì‹¤ë˜ìš”?)|
        (í•´[ì£¼ì‹œ]*ìˆ˜[\s]*ìˆë‚˜ìš”?)|
        (í•´[ì£¼ì‹œ]*ì…¨ìœ¼ë©´[\s]*ì¢‹ê² ì–´ìš”?)|
        (í•´[ì£¼ì‹œ]*ì‹­ì‹œì˜¤)|
        (í•´[ì£¼ì‹œ]*ì…”ìš”)|
        (í•´[ì£¼ì‹œ]*ê³ [\s]*ì‹¶ì–´ìš”)|
        (ë§í•´[\s]*ì¤˜)|
        (ë§í•´[\s]*ì£¼ì„¸ìš”)|
        (ë§í•´[ì£¼ì‹œ]*ê² ì–´ìš”?)|
        (ë§í•´[ì£¼ì‹œ]*ì‹¤ë˜ìš”?)|
        (ë§í•´[ì£¼ì‹œ]*ìˆ˜[\s]*ìˆë‚˜ìš”?)|
        (ë§í•´[ì£¼ì‹œ]*ì…¨ìœ¼ë©´[\s]*ì¢‹ê² ì–´ìš”?)|
        (ë§í•´[ì£¼ì‹œ]*ì‹­ì‹œì˜¤)|
        (ë§í•´[ì£¼ì‹œ]*ì…”ìš”)|
        (ë§í•´[ì£¼ì‹œ]*ê³ [\s]*ì‹¶ì–´ìš”)|
        (ì„¤ëª…í•´[\s]*ì¤˜)|
        (ì„¤ëª…í•´[\s]*ì£¼ì„¸ìš”)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ê² ì–´ìš”?)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ì‹¤ë˜ìš”?)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ìˆ˜[\s]*ìˆë‚˜ìš”?)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ì…¨ìœ¼ë©´[\s]*ì¢‹ê² ì–´ìš”?)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ì‹­ì‹œì˜¤)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ì…”ìš”)|
        (ì„¤ëª…í•´[ì£¼ì‹œ]*ê³ [\s]*ì‹¶ì–´ìš”)
    '''
    text = re.sub(pattern, '', text, flags=re.VERBOSE)

    # ë¶™ì„ ë° ë„ì–´ì“°ê¸° ë³€í˜•ê¹Œì§€ í•œ ë²ˆ ë” ë³´ì •
    text = re.sub(r'(ì•Œë ¤ ?ì¤˜|ì•Œë ¤ ?ì£¼ì„¸ìš”|ì•Œë ¤ ?ì£¼[ì„¸ìš”]*|ì•Œë ¤ ?ì£¼ì‹¤ë˜ìš”|ì•Œë ¤ ?ì£¼ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”?|ì•Œë ¤ ?ì£¼ì‹­ì‹œì˜¤|ì•Œë ¤ ?ì£¼ì…”ìš”|ì•Œë ¤ ?ì£¼ê³  ì‹¶ì–´ìš”|í•´ ?ì¤˜|í•´ ?ì£¼ì„¸ìš”|í•´ ?ì£¼ì‹¤ë˜ìš”|í•´ ?ì£¼ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”?|í•´ ?ì£¼ì‹­ì‹œì˜¤|í•´ ?ì£¼ê³  ì‹¶ì–´ìš”|ì¢€|ë§í•´ ?ì¤˜|ë§í•´ ?ì£¼ì„¸ìš”|ì„¤ëª…í•´ ?ì¤˜|ì„¤ëª…í•´ ?ì£¼ì„¸ìš”)', '', text)

    # 3. íŠ¹ìˆ˜ë¬¸ì, ë¬¼ìŒí‘œ, ì‰¼í‘œ ë“± ì œê±°
    text = re.sub(r'[?.,!\"\'\(\)\[\]\{\}]', '', text)

    # 4. ì´ì¤‘ê³µë°± ë° ì•ë’¤ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

async def get_market_analysis(user_query: str):
    print("[DEBUG] ì§„ì…, input:", user_query)
    cleaned_query = clean_korean_query(user_query)
    print("[DEBUG] cleaned_query:", cleaned_query)
    bright_data = await get_common(url=bright_data_url, query=cleaned_query)
    print("[DEBUG] get_common()ì˜ ì‹¤ì œ ë°˜í™˜ê°’ type:", type(bright_data))
    return bright_data