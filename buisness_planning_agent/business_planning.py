"""
Business Planning Agent - ê³µí†µ ëª¨ë“ˆ ìµœëŒ€ í™œìš© ë²„ì „
ëª¨ë“  shared_modules ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë¦¬íŒ©í† ë§
"""

import sys
import os
import time
from typing import Optional, List, Dict, Any
from datetime import datetime
import re

# í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™” (ChromaDB ì˜¤ë¥˜ ë°©ì§€)
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_TELEMETRY'] = 'False' 
os.environ['DO_NOT_TRACK'] = '1'

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from shared_modules.utils import get_or_create_conversation_session
# ê³µí†µ ëª¨ë“ˆì—ì„œ ëª¨ë“  í•„ìš”í•œ ê²ƒë“¤ import (ì´ì œ __init__.pyì—ì„œ exportë¨)
from shared_modules import (
    get_config, 
    get_llm_manager,
    get_vector_manager, 
    get_db_manager,
    get_session_context,
    setup_logging,
    create_conversation, 
    create_message, 
    get_conversation_by_id, 
    get_recent_messages,
    get_template_by_title,

    create_report,
    get_db_dependency,
    get_user_reports,
    get_report_by_id,

    insert_message_raw,
    load_prompt_from_file,
    create_success_response,
    create_error_response,
    format_conversation_history,
    sanitize_filename,
    get_current_timestamp,
    create_business_response,  # í‘œì¤€ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
)

# ğŸ”¥ í”„ë¡œì íŠ¸ ìë™ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
from shared_modules.project_utils import (
    save_business_plan_as_project,
    check_project_completion,
    auto_save_completed_project
)

sys.path.append(os.path.join(os.path.dirname(__file__), '../unified_agent_system'))
from core.models import UnifiedResponse, RoutingDecision, AgentType

from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import StrOutputParser
from fastapi import FastAPI, Body, HTTPException, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

# ë¡œê¹… ì„¤ì • - ê³µí†µ ëª¨ë“ˆ í™œìš©
logger = setup_logging("business_planning", log_file="logs/business_planning.log")

# ì„¤ì • ë¡œë“œ - ê³µí†µ ëª¨ë“ˆ í™œìš©
config = get_config()

# LLM, Vector, DB ë§¤ë‹ˆì € - ê³µí†µ ëª¨ë“ˆ í™œìš©
llm_manager = get_llm_manager()
vector_manager = get_vector_manager()
db_manager = get_db_manager()

# FastAPI ì´ˆê¸°í™”
app = FastAPI(
    title="Business Planning Agent",
    description="1ì¸ ì°½ì—… ì „ë¬¸ ì»¨ì„¤íŒ… ì—ì´ì „íŠ¸",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

from google_drive_service import google_drive_router
from draft import draft_router

app.include_router(google_drive_router)
app.include_router(draft_router)

# í”„ë¡¬í”„íŠ¸ ì„¤ì • - ê³µí†µ ëª¨ë“ˆì˜ ìœ í‹¸ë¦¬í‹° í™œìš©
try:
    from config.prompts_config import PROMPT_META
except ImportError:
    logger.warning("prompts_config.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    PROMPT_META = {}

from idea_market import get_persona_trend, get_market_analysis
from multi_turn import MultiTurnManager

class BusinessPlanningService:
    """ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ - ê³µí†µ ëª¨ë“ˆ ìµœëŒ€ í™œìš©"""

    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.llm_manager = llm_manager
        self.multi_turn = MultiTurnManager(self.llm_manager)
        self.vector_manager = vector_manager
        self.db_manager = db_manager
        
        # í† í”½ ë¶„ë¥˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.topic_classify_prompt = self._load_classification_prompt()
        
        logger.info("BusinessPlanningService ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_next_stage(self, current_stage: str) -> Optional[str]:
        try:
            idx = self.STAGES.index(current_stage)
            return self.STAGES[idx + 1] if idx + 1 < len(self.STAGES) else None
        except ValueError:
            return None
    
    async def is_single_question(self, user_input: str) -> bool:
        """
        ì‹±ê¸€í„´(single)ì¸ì§€ ë©€í‹°í„´(multi)ì¸ì§€ LLMìœ¼ë¡œ íŒë³„.
        - LLMì´ 'ë‹¨ê³„ ì£¼ì œ'ì™€ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨í•´ multi ì—¬ë¶€ë¥¼ ê²°ì •
        - ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ multi, ì•„ë‹ˆë©´ single/multi íŒë‹¨
        """
        try:
            judge_prompt = f"""
            ë‹¤ìŒ ì§ˆë¬¸ì´ ì•„ë˜ ë‹¨ê³„ ì£¼ì œì™€ ê´€ë ¨ì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ multië¥¼ ì¶œë ¥í•˜ì„¸ìš”.
            - ë‹¨ê³„ ì£¼ì œ: "ì•„ì´ë””ì–´ íƒìƒ‰ ë° ì¶”ì²œ", "ì‹œì¥ ê²€ì¦", "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ë§", "ì‹¤í–‰ ê³„íš ìˆ˜ë¦½", "ì„±ì¥ ì „ëµ & ë¦¬ìŠ¤í¬ ê´€ë¦¬"
            - ë‹¨ê³„ ì£¼ì œì™€ ì „í˜€ ê´€ë ¨ì´ ì—†ìœ¼ë©´ ì§ˆë¬¸ ë‚œì´ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ single ë˜ëŠ” multië¥¼ ì¶œë ¥í•˜ì„¸ìš”.
            - ì¦‰ë‹µ ê°€ëŠ¥í•œ ë‹¨ìˆœ ì •ë³´: "single"
            - ì „ëµ/ë¶„ì„/ë‹¨ê³„ë³„ ì„¤ëª… í•„ìš”: "multi"
            - ë‹µë³€ì€ ë°˜ë“œì‹œ single ë˜ëŠ” multië§Œ ì¶œë ¥.

            ì§ˆë¬¸: "{user_input}"
            """
            messages = [
                {"role": "system", "content": "ë„ˆëŠ” single/multië§Œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": judge_prompt}
            ]

            result = await self.llm_manager.generate_response(messages=messages, provider="openai")
            result_clean = result.strip().lower()

            return "single" in result_clean and "multi" not in result_clean
        except Exception as e:
            logger.error(f"[is_single_question] íŒë³„ ì‹¤íŒ¨: {e}")
            return False  # ì—ëŸ¬ ì‹œ ë©€í‹°í„´ ì§„í–‰


    # 11. final_business_plan(ì‚¬ì—… ê¸°íšì„œ ì‘ì„±)
    # 4. market_research(ì‹œì¥/ê²½ìŸ ë¶„ì„, ì‹œì¥ê·œëª¨)
    def _load_classification_prompt(self) -> str:
        """ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        return """
        ë„ˆëŠ” ê³ ê° ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ê´€ë ¨ëœ ì‚¬ì—…ê¸°íš topicì„ ëª¨ë‘ ê³¨ë¼ì£¼ëŠ” ì—­í• ì´ì•¼.
        ì†Œê´„í˜¸ ì•ˆì€ topicì— ëŒ€í•œ ë¶€ê°€ ì„¤ëª…ì´ì•¼. ë¶€ê°€ ì„¤ëª…ì„ ì°¸ê³ í•´ì„œ

        ì•„ë˜ì˜ í† í”½ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë¥¼ **ë³µìˆ˜ ê°œê¹Œì§€** ê³¨ë¼ì¤˜.

        ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ë§Œ ì¶œë ¥í•˜ê³ , ì„¤ëª…ì€ í•˜ì§€ë§ˆ.

        ê°€ëŠ¥í•œ í† í”½:
        0. startup_preparation(ì°½ì—… ì¤€ë¹„, ì²´í¬ë¦¬ìŠ¤íŠ¸)
        1. idea_recommendation(ì°½ì—… ì•„ì´í…œ, íŠ¸ë Œë“œ, ì•„ì´ë””ì–´ ì¶”ì²œ)
        2. idea_validation(ì•„ì´ë””ì–´ ê²€ì¦, ì‹œì¥ì„± ë¶„ì„, ì‹œì¥ê·œëª¨, íƒ€ê²Ÿ ë¶„ì„)
        3. business_model(ë¦°ìº”ë²„ìŠ¤, ìˆ˜ìµ êµ¬ì¡°)
        5. mvp_development(MVP ê¸°íš, ì´ˆê¸° ì œí’ˆ ì„¤ê³„)
        6. funding_strategy(íˆ¬ììœ ì¹˜, ì •ë¶€ì§€ì›, ìê¸ˆ ì¡°ë‹¬)
        7. business_registration(ì‚¬ì—…ìë“±ë¡, ë©´í—ˆ, ì‹ ê³  ì ˆì°¨)
        8. financial_planning(ì˜ˆì‚°, ë§¤ì¶œ, ì„¸ë¬´)
        9. growth_strategy(ì‚¬ì—… í™•ì¥, ìŠ¤ì¼€ì¼ì—…)
        10. risk_management(ë¦¬ìŠ¤í¬ ê´€ë¦¬, ìœ„ê¸° ëŒ€ì‘)
        11. final_business_plan(ì‚¬ì—… ê¸°íšì„œ ì‘ì„±)
        
        **ì¶œë ¥ ì˜ˆì‹œ**: startup_preparation, idea_validation
        """
    
    async def classify_topics(self, user_input: str) -> List[str]:
        """í† í”½ ë¶„ë¥˜ - ê³µí†µ ëª¨ë“ˆì˜ LLM ë§¤ë‹ˆì € í™œìš©"""
        try:
            messages = [
                {"role": "system", "content": self.topic_classify_prompt},
                {"role": "user", "content": f"ì‚¬ìš©ì ì§ˆë¬¸: {user_input}"}
            ]
            
            # ê³µí†µ ëª¨ë“ˆì˜ LLM ë§¤ë‹ˆì €ë¡œ ì‘ë‹µ ìƒì„±
            result = await self.llm_manager.generate_response(
                messages=messages,
                provider="openai"  # ë¶„ë¥˜ì—ëŠ” OpenAI ì‚¬ìš©
            )
            
            # ê²°ê³¼ íŒŒì‹±
            topics = [t.strip() for t in result.split(",") if t.strip() in PROMPT_META]
            logger.info(f"í† í”½ ë¶„ë¥˜ ê²°ê³¼: {topics}")
            return topics
            
        except Exception as e:
            logger.error(f"í† í”½ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return []
    
    def load_prompt_texts(self, topics: List[str]) -> List[str]:
        """í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ë¡œë“œ - ê³µí†µ ëª¨ë“ˆì˜ ìœ í‹¸ë¦¬í‹° í™œìš©"""
        merged_prompts = []
        
        for topic in topics:
            if topic in PROMPT_META:
                file_path = PROMPT_META[topic]["file"]
                # ê³µí†µ ëª¨ë“ˆì˜ load_prompt_from_file í™œìš©
                prompt_text = load_prompt_from_file(file_path)
                if prompt_text:
                    merged_prompts.append(prompt_text)
                    logger.info(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ: {topic}")
                else:
                    logger.warning(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {file_path}")
        
        return merged_prompts
    

    def build_agent_prompt(
        self, topics: List[str], user_input: str, persona: str, history: str,
        current_stage: str, progress: float, missing: List[str]
    ) -> PromptTemplate:
        """
        ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„± - LLMì´ ë¶€ì¡±í•œ ì •ë³´ë‚˜ ì§„í–‰ë¥ ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ í›„ì† ì§ˆë¬¸ì„ ìƒì„±.
        """
        merged_prompts = self.load_prompt_texts(topics)
        role_descriptions = [PROMPT_META[topic]["role"] for topic in topics if topic in PROMPT_META]

        # í˜ë¥´ì†Œë‚˜ë³„ ì»¨í…ìŠ¤íŠ¸
        if persona == "common":
            system_context = f"ë‹¹ì‹ ì€ 1ì¸ ì°½ì—… ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. {', '.join(role_descriptions)}"
        else:
            system_context = f"ë‹¹ì‹ ì€ {persona} ì „ë¬¸ 1ì¸ ì°½ì—… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. {', '.join(role_descriptions)}"

        # ì§„í–‰ë¥  ë° ë¶€ì¡± ì •ë³´ ì•ˆë‚´
        progress_hint = ""
        if progress >= 0.8:
            next_stage = self.multi_turn.get_next_stage(current_stage)
            if next_stage:
                progress_hint = f"\ní˜„ì¬ '{current_stage}' ë‹¨ê³„ë¥¼ ê±°ì˜ ë§ˆì³¤ìŠµë‹ˆë‹¤. ì´ì œ '{next_stage}' ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì œì•ˆí•˜ì„¸ìš”."
            else:
                progress_hint = "\nëª¨ë“  ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ìµœì¢… ì‚¬ì—…ê¸°íšì„œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŒì„ ìì—°ìŠ¤ëŸ½ê²Œ ì•Œë¦¬ì„¸ìš”."

        missing_hint = ""
        if missing:
            missing_hint = (
                f"\n'{current_stage}' ë‹¨ê³„ì—ì„œ ë¶€ì¡±í•œ ì •ë³´: {', '.join(missing)}. "
                f"ì´ ì •ë³´ë¥¼ ì•Œë ¤ì¤„ê¹Œìš”? ë˜ëŠ” ì‚¬ìš©ìì—ê²Œ í¥ë¯¸ë¥¼ ìœ ë„í•  ì§ˆë¬¸ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”."
            )

        template = f"""{system_context}

    ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:
    { " | ".join(merged_prompts) }

    ìµœê·¼ ëŒ€í™”:
    {history}

    ì°¸ê³  ë¬¸ì„œ:
    {{context}}

    ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"

    [ì‘ë‹µ ì§€ì¹¨]
    **ë¬¸ë‹¨ êµ¬ì„± ë° ê°€ë…ì„±:**
    - í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ê°„ë‹¨íˆ ìš”ì•½í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”
    - ë‚´ìš©ì„ ë…¼ë¦¬ì ì¸ ìˆœì„œë¡œ 2-4ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ êµ¬ì„±í•˜ì„¸ìš”
    - ê° ë¬¸ë‹¨ì€ í•˜ë‚˜ì˜ ì£¼ì œë§Œ ë‹¤ë£¨ê³ , ë¬¸ë‹¨ ê°„ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°ì„ ìœ ì§€í•˜ì„¸ìš”
    - ì¤‘ìš”í•œ í‚¤ì›Œë“œëŠ” **êµµê²Œ** í‘œì‹œí•˜ê³ , í•„ìš”ì‹œ ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¥¼ í™œìš©í•˜ì„¸ìš”
    - ë§ˆì§€ë§‰ì—ëŠ” êµ¬ì²´ì ì¸ ë‹¤ìŒ ë‹¨ê³„ë‚˜ ì§ˆë¬¸ì„ ì œì‹œí•˜ì„¸ìš”
    
    **í†¤ì•¤ë§¤ë„ˆ:**
    - ì „ë¬¸ì ì´ì§€ë§Œ ì¹œê·¼í•œ 1ì¸ ì°½ì—… ì»¨ì„¤í„´íŠ¸ ì–´ì¡° ìœ ì§€
    - ë³µì¡í•œ ë‚´ìš©ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…
    - ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ "ì¶”ê°€ë¡œ ì•Œì•„ë³´ì‹¤ê¹Œìš”?" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ ì œì•ˆ
    - ì§„í–‰ë¥ ì´ ë†’ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œì˜ ì „í™˜ì„ ë¶€ë“œëŸ½ê²Œ ì œì•ˆ
    {progress_hint}
    {missing_hint}
    """.strip()

        return PromptTemplate(
            input_variables=["context"],
            template=template
        )


    
        
    def format_history(self, messages: List[Any]) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ… - ê³µí†µ ëª¨ë“ˆ í™œìš©"""
        history_data = []
        for msg in reversed(messages):  # ì‹œê°„ìˆœ ì •ë ¬
            history_data.append({
                "role": "user" if msg.sender_type.lower() == "user" else "assistant",
                "content": msg.content,
                "timestamp": msg.created_at.isoformat() if msg.created_at else None,
                "agent_type": msg.agent_type
            })
        
        # ê³µí†µ ëª¨ë“ˆì˜ format_conversation_history í™œìš©
        return format_conversation_history(history_data, max_messages=10)
    
    async def _handle_special_topic(
            self, topic: str, persona: str, user_input: str, prompt: PromptTemplate,
            current_stage: str, progress: float, missing: List[str],
            next_stage: Optional[str], next_question: Optional[str]
        ):
        """idea_recommendation, idea_validation ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜"""
        logger.info("handle_special_topic ì‹œì‘")
        topic_data_funcs = {
            "idea_recommendation": get_persona_trend,
            "idea_validation": get_market_analysis,
        }
       
        get_data_func = topic_data_funcs.get(topic)
        if not get_data_func:
            raise ValueError(f"Unsupported topic for special handling: {topic}")

        logger.info(f"get_data_func type: {type(get_data_func)}, value: {get_data_func}")

        try:
            if topic == "idea_recommendation":
                logger.info(f"{get_data_func} ì‹¤í–‰")
                result = await get_data_func(persona, user_input)
                # get_persona_trendëŠ” íŠœí”Œ (trend_data, mcp_source)ë¥¼ ë°˜í™˜
                if isinstance(result, tuple) and len(result) == 2:
                    trend_data, mcp_source = result
                else:
                    trend_data = str(result)
                    mcp_source = "smithery_ai/persona-trend"
                logger.info(f"{get_data_func} ì‹¤í–‰ì™„ë£Œ")
            elif topic == "idea_validation":
                logger.info(f"{get_data_func} ì‹¤í–‰")
                trend_data = await get_data_func(user_input)
                # get_market_analysisëŠ” ë‹¨ì¼ ë¬¸ìì—´ì„ ë°˜í™˜
                mcp_source = "smithery_ai/brightdata-search"
                logger.info(f"{get_data_func} ì‹¤í–‰ì™„ë£Œ")
            else:
                raise ValueError("Unsupported topic")
        except Exception as e:
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            trend_data = "ì‹œì¥ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì°½ì—… ì»¨ì„¤íŒ… ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            mcp_source = "fallback"
        
        logger.info(f"trend_data type: {type(trend_data)}, value: {trend_data}")

        # LLM ì‘ë‹µ ìƒì„± (answer)
        prompt_str = prompt.template.format(context=trend_data)
        messages = [{"role": "user", "content": prompt_str}]
        logger.info(f"1ì°¨ ê¸°ë³¸ ìš”ì•½ : {messages}")

        answer = await self.llm_manager.generate_response(
            messages=messages,
            provider="openai",
        )
        
        return {
            "topics": [topic],
            "answer": answer,
            "sources": mcp_source,
            "retrieval_used": False,
            "metadata": {
                "type": topic,
                "current_stage": current_stage,
                "progress": progress,
                "missing": missing,
                "next_stage": next_stage,
                "next_question": next_question
            }
        }
    
    async def run_rag_query(
        self, conversation_id: int, user_input: str, use_retriever: bool = True, persona: str = "common"
    ) -> Dict[str, Any]:
        try:
            # 1. ëŒ€í™” íˆìŠ¤í† ë¦¬
            with get_session_context() as db:
                messages = get_recent_messages(db, conversation_id, 10)
                history = self.format_history(messages)

            # 2. í† í”½ ë° ë‹¨ê³„
            topics = await self.classify_topics(user_input)
            current_stage = self.multi_turn.determine_stage(topics)
            print("topics:",topics)
            print("current_stage:",current_stage)

            # 3. ì§„í–‰ë¥  ë° ëˆ„ë½ ì •ë³´
            progress_info = await self.multi_turn.check_overall_progress(conversation_id, history)
            progress = progress_info.get("current_progress", 0.0)
            missing = progress_info.get("missing", [])
            logger.info(f"progress: {progress}, missing: {missing}")
            
            # ğŸ”¥ ìƒˆë¡œìš´ ê¸°ëŠ¥: ìµœì¢… ê¸°íšì„œ ì‘ì„± ê°ì§€ ë° ìë™ ì €ì¥
            if current_stage == "ìµœì¢… ê¸°íšì„œ ì‘ì„±" or "final_business_plan" in topics or progress >= 0.95:
                logger.info(f"[AUTO_SAVE] ìµœì¢… ì‚¬ì—…ê¸°íšì„œ ì‘ì„± ë‹¨ê³„ ê°ì§€ - conversation_id: {conversation_id}, progress: {progress}")
                
                # ìµœì¢… ì‚¬ì—…ê¸°íšì„œ ìƒì„±
                business_plan_content = await self._generate_final_business_plan(conversation_id, history)
                
                # ğŸ”¥ ìë™ ì €ì¥ ì‹¤í–‰
                auto_saved = False
                save_message = ""
                
                try:
                    # ìˆ˜ì§‘ëœ ì •ë³´ ì¶”ì¶œ
                    business_info = self._extract_business_info_from_history(history)
                    # ì‚¬ìš©ì ID ì¶”ì¶œ (ëŒ€í™”ì—ì„œ)
                    with get_session_context() as db:
                        conversation = get_conversation_by_id(db, conversation_id)
                        if conversation:
                            user_id = conversation.user_id
                            
                            # í”„ë¡œì íŠ¸ë¡œ ìë™ ì €ì¥
                            save_result = save_business_plan_as_project(
                                user_id=user_id,
                                conversation_id=conversation_id,
                                business_plan_content=business_plan_content,
                                business_info=business_info
                            )
                            
                            if save_result["success"]:
                                logger.info(f"[AUTO_SAVE] ì‚¬ì—…ê¸°íšì„œ ìë™ ì €ì¥ ì„±ê³µ: project_id={save_result['project_id']}")
                                auto_saved = True
                                
                                # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
                                save_message = f"\n\nğŸ“ **ìë™ ì €ì¥ ì™„ë£Œ**\n" \
                                            f"â€¢ í”„ë¡œì íŠ¸ ì œëª©: {save_result['title']}\n" \
                                            f"â€¢ íŒŒì¼ëª…: {save_result['file_name']}\n" \
                                            f"â€¢ í”„ë¡œì íŠ¸ ID: {save_result['project_id']}\n" \
                                            f"â€¢ ì €ì¥ ì‹œê°„: {get_current_timestamp()}\n\n" \
                                            f"ğŸ’¡ ë§ˆì´í˜ì´ì§€ì—ì„œ ì €ì¥ëœ ì‚¬ì—…ê¸°íšì„œë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                            else:
                                logger.error(f"[AUTO_SAVE] ì‚¬ì—…ê¸°íšì„œ ìë™ ì €ì¥ ì‹¤íŒ¨: {save_result.get('error')}")
                                save_message = f"\n\nâš ï¸ **ì €ì¥ ì‹¤íŒ¨**\nì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {save_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                        else:
                            logger.error(f"[AUTO_SAVE] ëŒ€í™” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: conversation_id={conversation_id}")
                            save_message = "\n\nâš ï¸ **ì €ì¥ ì‹¤íŒ¨**\nëŒ€í™” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                
                except Exception as save_error:
                    logger.error(f"[AUTO_SAVE] ìë™ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {save_error}")
                    save_message = f"\n\nâš ï¸ **ì €ì¥ ì‹¤íŒ¨**\nì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(save_error)}"
                
                # ìµœì¢… ì‘ë‹µ êµ¬ì„±
                final_content = business_plan_content + save_message
                
                return {
                    "topics": topics,
                    "answer": final_content,
                    "sources": "ì¢…í•©ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸°ë°˜",
                    "retrieval_used": False,
                    "metadata": {
                        "type": "final_business_plan",
                        "current_stage": current_stage,
                        "progress": 1.0,  # ì™„ë£Œ
                        "missing": [],
                        "next_stage": None,
                        "next_question": None,
                        "auto_saved": auto_saved,
                        "completion_detected": True
                    }
                }

            # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.build_agent_prompt(topics, user_input, persona, history, current_stage, progress, missing)

            # ğŸ”¥ íŠ¹ë³„í•œ í† í”½ ì²˜ë¦¬ ì¶”ê°€
            special_topics = ["idea_recommendation", "idea_validation"]
            if any(topic in special_topics for topic in topics):
                special_topic = next(topic for topic in topics if topic in special_topics)
                next_stage = self.multi_turn.get_next_stage(current_stage)
                
                return await self._handle_special_topic(
                    topic=special_topic,
                    persona=persona,
                    user_input=user_input,
                    prompt=prompt,
                    current_stage=current_stage,
                    progress=progress,
                    missing=missing,
                    next_stage=next_stage,
                    next_question=None
                )

            # 5. RAG or Fallback
            if use_retriever and topics:
                try:
                    topic_filter = {"$and": [{"category": "business_planning"}, {"topic": {"$in": topics}}]}
                    retriever = self.vector_manager.get_retriever(
                        collection_name="global-documents",
                        k=5,
                        search_kwargs={"filter": topic_filter}
                    )

                    if retriever:
                        llm = self.llm_manager.get_llm(load_balance=True)
                        qa_chain = RetrievalQA.from_chain_type(
                            llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            chain_type_kwargs={"prompt": prompt},
                            return_source_documents=True
                        )

                        result = qa_chain.invoke(user_input) or {}
                        sources = self._format_source_documents(result.get("source_documents", []))

                        return {
                            "topics": topics,
                            "answer": result.get('result', "ê´€ë ¨ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                            "sources": sources,
                            "retrieval_used": True,
                            "metadata": {
                                "type": topics[0] if topics else "general",
                                "current_stage": current_stage,
                                "progress": progress,
                                "missing": missing,
                                "next_stage": self.multi_turn.get_next_stage(current_stage),
                                "next_question": None
                            }
                        }
                    else:
                        logger.warning("Retriever ìƒì„± ì‹¤íŒ¨, í´ë°±ìœ¼ë¡œ ì „í™˜")

                except Exception as e:
                    logger.warning(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. í´ë°±ìœ¼ë¡œ ì „í™˜: {e}")

            # Fallback ê²½ë¡œ
            return await self._generate_fallback_response(
                topics, user_input, prompt, current_stage, None, progress, missing
            )

        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return await self._generate_fallback_response([], user_input, self.build_agent_prompt([], user_input, persona, "", "ì•„ì´ë””ì–´ íƒìƒ‰", 0.0, []))

    async def _generate_final_business_plan(self, conversation_id: int, history: str) -> str:
        """
        ìµœì¢… ì‚¬ì—…ê¸°íšì„œë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìš”ì•½í•˜ê³  ë¬¸ì„œí™”
        """
        try:
            messages = [
                {"role": "system", "content": "ë„ˆëŠ” 1ì¸ ì°½ì—… ì „ë¬¸ê°€ë¡œì„œ ì‚¬ì—…ê¸°íšì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": f"""
ë‹¤ìŒ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•©ì ì¸ ì‚¬ì—…ê¸°íšì„œë¥¼ ì‘ì„±í•´ì¤˜:

{history}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

# ğŸ“‹ ì‚¬ì—…ê¸°íšì„œ

## 1. ì‚¬ì—… ê°œìš”
- ì‚¬ì—… ì•„ì´ë””ì–´
- ì‚¬ì—… ëª©í‘œ
- í•µì‹¬ ê°€ì¹˜ ì œì•ˆ

## 2. ì‹œì¥ ë¶„ì„
- ì‹œì¥ ê·œëª¨ ë° ë™í–¥
- íƒ€ê²Ÿ ê³ ê° ë¶„ì„
- ê²½ìŸì‚¬ ë¶„ì„

## 3. ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸
- ìˆ˜ìµ ëª¨ë¸
- ê°€ê²© ì „ëµ
- ë¹„ìš© êµ¬ì¡°

## 4. ì‹¤í–‰ ê³„íš
- ê°œë°œ ë¡œë“œë§µ
- ë§ˆì¼€íŒ… ê³„íš
- ìš´ì˜ ê³„íš

## 5. ì¬ë¬´ ê³„íš
- ì´ˆê¸° íˆ¬ìë¹„
- ë§¤ì¶œ ì˜ˆìƒ
- ì†ìµ ë¶„ì„

## 6. ë¦¬ìŠ¤í¬ ê´€ë¦¬
- ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸
- ëŒ€ì‘ ë°©ì•ˆ
- ë°±ì—… ê³„íš

## 7. í–¥í›„ ê³„íš
- ë‹¨ê¸° ëª©í‘œ (6ê°œì›”)
- ì¤‘ê¸° ëª©í‘œ (1-2ë…„)
- ì¥ê¸° ë¹„ì „ (3-5ë…„)

ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ê°œí–‰ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""}
            ]
            result = await self.llm_manager.generate_response(messages=messages, provider="openai")
            return result
        except Exception as e:
            logger.error(f"[final_business_plan] ìƒì„± ì‹¤íŒ¨: {e}")
            return "ìµœì¢… ì‚¬ì—…ê¸°íšì„œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _extract_business_info_from_history(self, history: str) -> Dict[str, Any]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        business_info = {}
        
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
            history_lower = history.lower()
            
            # ì—…ì¢… ì¶”ì¶œ
            business_types = ["ì¹´í˜", "ì‡¼í•‘ëª°", "ë·°í‹°", "êµìœ¡", "IT", "ì‹ë‹¹", "ì„œë¹„ìŠ¤", "ì œì¡°", "ìœ í†µ"]
            for bt in business_types:
                if bt in history:
                    business_info["business_type"] = bt
                    break
            
            # ëª©í‘œ ì¶”ì¶œ
            if "ë§¤ì¶œ" in history_lower or "ìˆ˜ìµ" in history_lower:
                business_info["main_goal"] = "ë§¤ì¶œ ì¦ëŒ€"
            elif "ê³ ê°" in history_lower or "ì¸ì§€ë„" in history_lower:
                business_info["main_goal"] = "ê³ ê° í™•ë³´"
            
            # íƒ€ê²Ÿ ì¶”ì¶œ
            age_groups = ["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€"]
            for age in age_groups:
                if age in history:
                    business_info["target_audience"] = age
                    break
            
            logger.info(f"ì¶”ì¶œëœ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´: {business_info}")
            return business_info
            
        except Exception as e:
            logger.error(f"ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}



    def _format_source_documents(self, documents: List[Any]) -> str:
        """ì†ŒìŠ¤ ë¬¸ì„œ í¬ë§·íŒ…"""
        if not documents:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        sources = []
        for doc in documents:
            source_info = {
                "source": doc.metadata.get("source", "âŒ ì—†ìŒ"),
                "metadata": doc.metadata,
                "length": len(doc.page_content),
                "snippet": doc.page_content[:300]
            }
            sources.append(f"# ë¬¸ì„œ\n{source_info['snippet']}\n")
        
        return "\n\n".join(sources)
    
    async def _generate_fallback_response(self, topics: List[str], user_input: str, prompt: PromptTemplate, current_stage: str = "ì•„ì´ë””ì–´ íƒìƒ‰", next_stage: str = "", progress: float = 0.0, missing: List[str] = []) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        try:
            # Geminië¡œ í´ë°± ì‘ë‹µ ìƒì„±
            llm = self.llm_manager.get_llm("gemini", load_balance=True)
            
            formatted_prompt = prompt.format(
                context="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì»¨ì„¤í„´íŠ¸ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                current_stage=current_stage or "ì•„ì´ë””ì–´ íƒìƒ‰",
                next_stage=next_stage or ""
            )
            
            messages = [{"role": "user", "content": formatted_prompt}]
            result = await self.llm_manager.generate_response(messages, provider="gemini")
            
            return {
                "topics": topics,
                "answer": result,
                "sources": "ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.",
                "retrieval_used": False,
                "metadata": {
                    "type": topics[0] if topics else "general",
                    "current_stage": current_stage,
                    "progress": progress,
                    "missing": missing,
                    "next_stage": next_stage,
                    "next_question": None
                }
            }
            
        except Exception as e:
            logger.error(f"í´ë°± ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return await self._generate_error_fallback(user_input)
    
    async def _generate_error_fallback(self, user_input: str) -> Dict[str, Any]:
        """ì—ëŸ¬ í´ë°± ì‘ë‹µ"""
        return {
            "topics": [],
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "sources": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "retrieval_used": False,
            "error": True
        }
    
    def handle_lean_canvas_request(self, user_question: str) -> Dict[str, Any]:
        """ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ì²˜ë¦¬ - ê³µí†µ ëª¨ë“ˆì˜ DB í•¨ìˆ˜ í™œìš©"""
        try:
            # ê¸°ë³¸ê°’: Common
            template_title = "ë¦° ìº”ë²„ìŠ¤_common"

            # ì„¸ë¶€ í‚¤ì›Œë“œì— ë”°ë¼ íƒ€ì´í‹€ ì§€ì •
            if "ë„¤ì¼" in user_question:
                template_title = "ë¦° ìº”ë²„ìŠ¤_nail"
            elif "ì†ëˆˆì¹" in user_question:
                template_title = "ë¦° ìº”ë²„ìŠ¤_eyelash"
            elif "ì‡¼í•‘ëª°" in user_question:
                template_title = "ë¦° ìº”ë²„ìŠ¤_ecommers"
            elif "ìœ íŠœë²„" in user_question or "í¬ë¦¬ì—ì´í„°" in user_question:
                template_title = "ë¦° ìº”ë²„ìŠ¤_creator"

            # ê³µí†µ ëª¨ë“ˆì˜ DB í•¨ìˆ˜ë¡œ í…œí”Œë¦¿ ì¡°íšŒ
            template = get_template_by_title(template_title)

            return {
                "type": "lean_canvas",
                "title": template_title,
                "content": template["content"] if template else "<p>í…œí”Œë¦¿ ì—†ìŒ</p>"
            }
            
        except Exception as e:
            logger.error(f"ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response("ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "TEMPLATE_LOAD_ERROR")

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
business_service = BusinessPlanningService()

# ìš”ì²­ ëª¨ë¸ ì •ì˜
class UserQuery(BaseModel):
    """ì‚¬ìš©ì ì¿¼ë¦¬ ìš”ì²­"""
    user_id: Optional[int] = Field(..., description="ì‚¬ìš©ì ID")
    conversation_id: Optional[int] = Field(None, description="ëŒ€í™” ID")
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
    persona: Optional[str] = Field(default="common", description="í˜ë¥´ì†Œë‚˜")
    
    class Config:
        json_schema_extra = {
            "example": {
                "user_id": 123,
                "conversation_id": 456,
                "message": "ì°½ì—… ì•„ì´ë””ì–´ë¥¼ ê²€ì¦í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
                "persona": "common"
            }
        }

# FastAPI ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/agent/query")
async def process_user_query(request: UserQuery):
    """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ê³µí†µ ëª¨ë“ˆë“¤ ìµœëŒ€ í™œìš©"""
    try:
        start_time = time.time()
        logger.info(f"[START] ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘ - user_id: {request.user_id}")

        user_question = request.message
        user_id = request.user_id
        conversation_id = request.conversation_id

        # 1. ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬
        logger.info("[STEP 1] ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬ ì‹œì‘")
        try:
            session_info = get_or_create_conversation_session(user_id, conversation_id)
            conversation_id = session_info["conversation_id"]
            logger.info("[STEP 1] ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"[STEP 1] ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response("ëŒ€í™” ì„¸ì…˜ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤", "SESSION_CREATE_ERROR")

        # 2. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        logger.info("[STEP 2] ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ì‹œì‘")
        try:
            with get_session_context() as db:
                user_message = create_message(db, conversation_id, "user", "business_planning", user_question)
                if not user_message:
                    logger.warning("[STEP 2] ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨")
            logger.info("[STEP 2] ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[STEP 2] ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {e}")

        # 3. ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ë¶„ê¸°
        logger.info("[STEP 3] ë¦°ìº”ë²„ìŠ¤ ì—¬ë¶€ í™•ì¸")
        if "ë¦°ìº”ë²„ìŠ¤" in user_question:
            logger.info("[STEP 3] ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ë¶„ê¸° ì§„ì…")
            lean_canvas_start = time.time()
            lean_canvas_result = business_service.handle_lean_canvas_request(user_question)
            logger.info(f"[STEP 3] ë¦°ìº”ë²„ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {time.time() - lean_canvas_start:.2f}s")

            response_data = UnifiedResponse(
                conversation_id=conversation_id,
                agent_type=AgentType.BUSINESS_PLANNING,
                response=lean_canvas_result["content"],
                confidence=0.9,
                routing_decision=RoutingDecision(
                    agent_type=AgentType.BUSINESS_PLANNING,
                    confidence=0.9,
                    reasoning="ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ìš”ì²­",
                    keywords=["ë¦°ìº”ë²„ìŠ¤", lean_canvas_result["title"]]
                ),
                sources=None,
                metadata={
                    "type": "lean_canvas",
                    "template_title": lean_canvas_result["title"]
                },
                processing_time=time.time() - start_time
            )
            return create_success_response(response_data)

        # 4. ì¼ë°˜ RAG ì¿¼ë¦¬ ì²˜ë¦¬
        logger.info("[STEP 4] RAG ì¿¼ë¦¬ ìš”ì²­ ì‹œì‘")
        rag_start = time.time()
        result = await business_service.run_rag_query(
            conversation_id,
            user_question,
            use_retriever=True,
            persona=request.persona or "common"
        )
        logger.info(f"[STEP 4] RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {time.time() - rag_start:.2f}s")

        # 5. ì—ì´ì „íŠ¸ ì‘ë‹µ ì €ì¥
        logger.info("[STEP 5] ì—ì´ì „íŠ¸ ì‘ë‹µ ì €ì¥ ì‹œì‘")
        try:
            content_to_save = result["answer"]

            # # ê¸°íšì„œ ì“¸ ë•Œ ë„ì›€ë˜ëŠ” íŠ¸ë Œë“œ/ì‹œì¥ ë°ì´í„°ëŠ” ì €ì¥
            # if result.get("sources") and any(t in ["idea_validation", "idea_recommendation"] for t in result.get("topics", [])):
            #     content_to_save += "\n\n[ì°¸ê³ ë¬¸ì„œ]\n" + str(result["sources"])

            insert_message_raw(
                conversation_id=conversation_id,
                sender_type="agent",
                agent_type="business_planning",
                content=content_to_save
            )
            logger.info("[STEP 5] ì—ì´ì „íŠ¸ ì‘ë‹µ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[STEP 5] ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {e}")

        # 6. ì‘ë‹µ ìƒì„±
        logger.info("[STEP 6] ì‘ë‹µ ìƒì„± ë° ë°˜í™˜")
        # if "metadata" in result and result["metadata"]:  # metadataê°€ ìˆì„ ê²½ìš°
        #     response_data = UnifiedResponse(
        #     conversation_id=conversation_id,
        #     agent_type=AgentType.BUSINESS_PLANNING,
        #     response=result["answer"],
        #     confidence=0.85,  # ìƒí™©ì— ë§ê²Œ ì§€ì •, í•„ìš”ì‹œ ê³„ì‚°
        #     routing_decision=RoutingDecision(
        #         agent_type=AgentType.BUSINESS_PLANNING,
        #         confidence=0.85,
        #         reasoning="ì‚¬ì—…ê¸°íšì„œ ì œê³µ",
        #         keywords=result.get("topics", [])
        #         ),
        #     sources=result.get("sources"),
        #     metadata=result["metadata"],
        #     processing_time=time.time() - start_time
        #     )
        #     return create_success_response(response_data)
            
        
        # response_data = create_business_response(
        #     conversation_id=conversation_id,
        #     answer=result["answer"],
        #     topics=result.get("topics", []),
        #     sources=result.get("sources", "")
        # )
        # logger.info(f"ì‘ë‹µì„ create_success_responseì— ë„£ê¸° ì „ :{response_data}")
        # logger.info(f"[END] ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ - ì´ ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}s")

        metadata = result.get("metadata", {})

        # if metadata.get("type") == "final_business_plan":
        #     metadata = {**metadata, "content": result["answer"]}
        #     result["answer"] = "ìµœì¢… ì‚¬ì—…ê¸°íšì„œê°€ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."

        response_data = UnifiedResponse(
            conversation_id=conversation_id,
            agent_type=AgentType.BUSINESS_PLANNING,
            response=result["answer"],
            confidence=result.get("confidence", 0.8),
            routing_decision=RoutingDecision(
                agent_type=AgentType.BUSINESS_PLANNING,
                confidence=result.get("confidence", 0.8),
                reasoning="ì‚¬ì—…ê¸°íš ë‹¨ê³„ë³„ ë©€í‹°í„´ ì§„í–‰",
                keywords=result.get("topics", [])
            ),
            sources=result.get("sources"),
            metadata=metadata,
            processing_time=time.time() - start_time
        )

        return create_success_response(response_data)

    except Exception as e:
        logger.error(f"[ERROR] ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return create_error_response(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "QUERY_PROCESSING_ERROR")

# @app.get("/lean_canvas/{title}")
# def preview_template(title: str):
#     """ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸° - í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš© ê¶Œì¥"""
#     return create_error_response("ì´ APIëŠ” í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤. í†µí•© ì‹œìŠ¤í…œì˜ /lean_canvas/{title}ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.", "API_MOVED")

@app.get("/lean_canvas/{title}")
def preview_template(title: str):
    """ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸° - ê³µí†µ ëª¨ë“ˆ í™œìš©"""
    try:
        # sanitize_filenameìœ¼ë¡œ ì•ˆì „í•œ íŒŒì¼ëª… ë³´ì¥
        #safe_title = sanitize_filename(title)
        
        # ê³µí†µ ëª¨ë“ˆì˜ DB í•¨ìˆ˜ë¡œ í…œí”Œë¦¿ ì¡°íšŒ
        template = get_template_by_title(title)
        html = template["content"] if template else "<p>í…œí”Œë¦¿ ì—†ìŒ</p>"
        
        return Response(content=html, media_type="text/html")
        
    except Exception as e:
        logger.error(f"í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")
        """ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸° - í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš© ê¶Œì¥"""
        return create_error_response("ì´ APIëŠ” í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤. í†µí•© ì‹œìŠ¤í…œì˜ /lean_canvas/{title}ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.", "API_MOVED")
    
@app.get("/health")
def health_check():
    """ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ - ê³µí†µ ëª¨ë“ˆë“¤ì˜ ìƒíƒœ ì²´í¬"""
    try:
        # ê° ë§¤ë‹ˆì €ì˜ ìƒíƒœ í™•ì¸
        config_status = config.validate_config()
        llm_status = llm_manager.get_status()
        vector_status = vector_manager.get_status()
        db_status = db_manager.test_connection()
        
        health_data = {
            "service": "business_planning_agent",
            "status": "healthy",
            "timestamp": get_current_timestamp(),
            "config": config_status,
            "llm": {
                "available_models": llm_status["available_models"],
                "current_provider": llm_status["current_provider"],
                "call_count": llm_status["call_count"]
            },
            "vector": {
                "embedding_available": vector_status["embedding_available"],
                "default_collection": vector_status["default_collection"],
                "cached_vectorstores": vector_status["cached_vectorstores"]
            },
            "database": {
                "connected": db_status,
                "engine_info": db_manager.get_engine_info()
            }
        }
        
        return create_success_response(health_data)
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return create_error_response(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {str(e)}", "HEALTH_CHECK_ERROR")

@app.get("/status/detailed")
def detailed_status():
    """ìƒì„¸ ìƒíƒœ í™•ì¸ - ê° ê³µí†µ ëª¨ë“ˆì˜ ìƒì„¸ ì •ë³´"""
    try:
        # LLM ì—°ê²° í…ŒìŠ¤íŠ¸
        llm_test = llm_manager.test_connection()
        
        # ë²¡í„° ìŠ¤í† ì–´ ì •ë³´
        vector_info = vector_manager.get_collection_info()
        
        detailed_data = {
            "service_info": {
                "name": "Business Planning Agent",
                "version": "2.0.0",
                "uptime": get_current_timestamp()
            },
            "llm_test_results": llm_test,
            "vector_collection_info": vector_info,
            "config_summary": config.to_dict(),
            "prompt_meta_available": len(PROMPT_META) > 0,
            "prompt_topics": list(PROMPT_META.keys()) if PROMPT_META else []
        }
        
        return create_success_response(detailed_data)
        
    except Exception as e:
        logger.error(f"ìƒì„¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return create_error_response(f"ìƒì„¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}", "DETAILED_STATUS_ERROR")


### pdf ë‹¤ìš´ë¡œë“œ ì¶”ê°€ ###
from fpdf import FPDF
from io import BytesIO
import uuid
import tempfile
import pdfkit

def generate_pdf_from_html(html_content: str) -> bytes:
    pdf_bytes = pdfkit.from_string(html_content, False)  # False: ë©”ëª¨ë¦¬ì— ì €ì¥
    return pdf_bytes


def generate_pdf(content: str) -> bytes:
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, content)
    pdf_output = BytesIO()
    pdf.output(pdf_output)
    return pdf_output.getvalue()

def save_pdf_to_temp(pdf_bytes: bytes) -> str:
    file_id = str(uuid.uuid4())
    temp_path = os.path.join(tempfile.gettempdir(), f"{file_id}.pdf")
    with open(temp_path, "wb") as f:
        f.write(pdf_bytes)
    return file_id

def load_pdf_from_temp(file_id: str) -> bytes:
    temp_path = os.path.join(tempfile.gettempdir(), f"{file_id}.pdf")
    with open(temp_path, "rb") as f:
        return f.read()

### pdf ìƒì„±/ë‹¤ìš´ë¡œë“œ api###
from fastapi.responses import StreamingResponse, JSONResponse

# @app.post("/report/pdf/create")
# async def create_pdf_report(data: dict = Body(...)):
#     content = data.get("content", "ë¦¬í¬íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
#     pdf_bytes = generate_pdf(content)
#     file_id = save_pdf_to_temp(pdf_bytes)
#     return JSONResponse({"file_id": file_id})

class PdfCreateRequest(BaseModel):
    html: str
    form_data: Optional[Dict[str, str]] = None
    user_id: int                       
    conversation_id: Optional[int] = None
    title: Optional[str] = "ë¦° ìº”ë²„ìŠ¤_common" 

## dbì— ì €ì¥
@app.post("/report/pdf/create")
async def create_pdf_from_html_api(data: PdfCreateRequest,
    db: Session = Depends(get_db_dependency),):
    html = data.html or "<p>ë‚´ìš© ì—†ìŒ</p>"
    form_data = data.form_data or {}
   
    try:
        pdf_bytes = generate_pdf_from_html(html)
        file_id = save_pdf_to_temp(pdf_bytes)
        file_url = f"/report/pdf/download/{file_id}"  # ìƒëŒ€ê²½ë¡œë¡œ ì €ì¥

        report = create_report(
            db=db,
            user_id=data.user_id,  
            conversation_id=data.conversation_id,
            report_type="ë¦°ìº”ë²„ìŠ¤",
            title=data.title, # í”„ë¡ íŠ¸ì—ì„œ ì£¼ëŠ” ê°’ ë°”ê¿”ì•¼í•¨
            content_data=form_data,  # JSONìœ¼ë¡œ ì €ì¥
            file_url=file_url,
        )
        if not report:
            raise Exception("DB ì €ì¥ ì‹¤íŒ¨")
        return JSONResponse({"file_id": file_id})
    except Exception as e:
        logger.error(f"PDF ìƒì„± ì‹¤íŒ¨: {e}")
    raise HTTPException(status_code=500, detail="PDF ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@app.get("/report/pdf/download/{file_id}")
async def download_pdf_report(file_id: str):
    pdf_bytes = load_pdf_from_temp(file_id)
    return StreamingResponse(BytesIO(pdf_bytes), media_type="application/pdf", headers={
        "Content-Disposition": f"attachment; filename=report_{file_id}.pdf"
    })

# ë¦¬í¬íŠ¸ ì¡°íšŒ
@app.get("/reports/{report_id}")
def get_report_detail(report_id: int, db: Session = Depends(get_db_dependency)):
    """
    ë¦¬í¬íŠ¸ ìƒì„¸ ì¡°íšŒ API
    """
    report = get_report_by_id(db, report_id)

    if not report:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    return {
        "success": True,
        "data": {
            "report_id": report.report_id,
            "report_type": report.report_type,
            "title": report.title,
            "status": "completed" if report.file_url else "generating",
            "content_data": report.content_data,
            "file_url": report.file_url,
            "created_at": report.created_at.isoformat()
        }
    }


@app.get("/reports")
def get_report_list(
    user_id: int = Query(...),  #  í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    report_type: Optional[str] = Query(None),
    status: Optional[str] = Query(None),
    db: Session = Depends(get_db_dependency)
):
    """
    ë¦¬í¬íŠ¸ ëª©ë¡ ì¡°íšŒ API (í•„ìˆ˜: user_id, ì„ íƒ: report_type, status)
    """
    try:
        reports = get_user_reports(db, user_id=user_id, report_type=report_type, limit=100)

        if status:
            if status == "completed":
                reports = [r for r in reports if r.file_url]
            elif status == "generating":
                reports = [r for r in reports if not r.file_url]

        return {
            "success": True,
            "data": [
                {
                    "report_id": r.report_id,
                    "report_type": r.report_type,
                    "title": r.title,
                    "status": "completed" if r.file_url else "generating",
                    "file_url": r.file_url,
                    "created_at": r.created_at.isoformat()
                }
                for r in reports
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# âœ… MCP í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ìš”ì²­ ëª¨ë¸
class McpPersonaTrendRequest(BaseModel):
    persona: str = Field(..., description="ì‚¬ìš©ì í˜ë¥´ì†Œë‚˜ (e_commerce, developer, creator ë“±)")
    query: str = Field(..., description="ë¶„ì„ìš© í‚¤ì›Œë“œ ë˜ëŠ” ì§ˆë¬¸")

# âœ… MCP ì‹œì¥ ë¶„ì„ ìš”ì²­ ëª¨ë¸
class McpMarketAnalysisRequest(BaseModel):
    query: str = Field(..., description="ì‹œì¥ ê·œëª¨/ê²½ìŸì‚¬ ë“± ììœ  ì§ˆë¬¸")
    
# âœ… MCP í˜ë¥´ì†Œë‚˜ íŠ¸ë Œë“œ ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/mcp/persona-trend")
async def get_mcp_persona_trend_api(request: McpPersonaTrendRequest):
    """
    MCP ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ë³„ íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ
    - ì˜ˆ: ì•„ë§ˆì¡´ ì¸ê¸° ìƒí’ˆ, ì•±ìŠ¤í† ì–´ ì‹ ê·œ ì•±, ìœ íŠœë¸Œ íŠ¸ë Œë“œ ë“±
    """
    try:
        # ğŸ“¡ idea_market.pyì˜ MCP í•¨ìˆ˜ í˜¸ì¶œ
        trend, source = await get_persona_trend(request.persona, request.query)

        return {
            "success": True,
            "trend": trend,
            "source_type": source  # ex: "smithery_ai/amazon-product-search"
        }
    except Exception as e:
        logger.error(f"[MCP PersonaTrend] ì˜¤ë¥˜ ë°œìƒ: {e}")
        return create_error_response("í˜ë¥´ì†Œë‚˜ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨", "MCP_ERROR")


# âœ… MCP ì‹œì¥ ë¶„ì„ API ì—”ë“œí¬ì¸íŠ¸
@app.post("/mcp/market-analysis")
async def get_mcp_market_analysis_api(request: McpMarketAnalysisRequest):
    """
    MCP ê¸°ë°˜ ì‹œì¥ ë¶„ì„ API
    - ë¸Œë¼ì´íŠ¸ë°ì´í„° ê¸°ë°˜ ê²½ìŸì‚¬/ì‹œì¥ê·œëª¨ ë¶„ì„ìš©
    """
    try:
        # ğŸ“¡ idea_market.pyì˜ get_market_analysis í˜¸ì¶œ
        result = await get_market_analysis(request.query)

        return {
            "success": True,
            "analysis": result,
            "source_type": "smithery_ai/brightdata-search"
        }
    except Exception as e:
        logger.error(f"[MCP MarketAnalysis] ì˜¤ë¥˜ ë°œìƒ: {e}")
        return create_error_response("ì‹œì¥ ë¶„ì„ ì‹¤íŒ¨", "MCP_ERROR")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    
    logger.info("=== Business Planning Agent v2.0 ì‹œì‘ ===\nâœ… ì´ì œ í†µí•© ì‹œìŠ¤í…œê³¼ ì—°ë™ë©ë‹ˆë‹¤.")
    logger.info("âœ… í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€: /agent/query, /health, /status")
    logger.info("âœ… ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ì€ í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©")
    
    uvicorn.run(
        app, 
        host=config.HOST, 
        port=config.PORT,
        log_level=config.LOG_LEVEL.lower()
    )

# ì‹¤í–‰ ëª…ë ¹ì–´:
# uvicorn business_planning:app --reload --host 0.0.0.0 --port 8080
# http://127.0.0.1:8080/docs

# python -m uvicorn business_planning:app --reload --host 0.0.0.0 --port 8001